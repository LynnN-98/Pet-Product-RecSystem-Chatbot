{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d94c5c4-a7d2-4388-863a-252e1afdaf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Using cached surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
      "Collecting scikit-surprise (from surprise)\n",
      "  Using cached scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.14.1)\n",
      "Using cached surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Installing collected packages: scikit-surprise, surprise\n",
      "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc14ef83-d7e9-4825-9aa6-a10799665c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (set): The set of items that the user actually interacted with or purchased.\n",
    "        predicted (list): The list of items recommended to the user, ranked by relevance.\n",
    "        k (int): The number of top items to consider for the calculation.\n",
    "    \n",
    "    Returns:\n",
    "        float: The Precision@k score, which measures the proportion of recommended items\n",
    "               in the top-k list that are relevant (i.e., present in the actual set).\n",
    "    \"\"\"\n",
    "    # Convert actual interactions to a set for faster membership testing\n",
    "    actual_set = set(actual)\n",
    "    \n",
    "    # Select the top-k predicted items and convert to a set\n",
    "    predicted_set = set(predicted[:k])\n",
    "    \n",
    "    # Handle edge case where no predictions are made\n",
    "    if len(predicted_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate precision as the ratio of relevant recommended items in the top-k list\n",
    "    return len(actual_set & predicted_set) / float(k)\n",
    "\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (set): The set of items that the user actually interacted with or purchased.\n",
    "        predicted (list): The list of items recommended to the user, ranked by relevance.\n",
    "        k (int): The number of top items to consider for the calculation.\n",
    "    \n",
    "    Returns:\n",
    "        float: The Recall@k score, which measures the proportion of relevant items\n",
    "               from the actual set that are included in the top-k recommended list.\n",
    "    \"\"\"\n",
    "    # Convert actual interactions to a set for faster membership testing\n",
    "    actual_set = set(actual)\n",
    "    \n",
    "    # Select the top-k predicted items and convert to a set\n",
    "    predicted_set = set(predicted[:k])\n",
    "    \n",
    "    # Handle edge case where the actual set is empty\n",
    "    if len(actual_set) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate recall as the ratio of relevant items retrieved from the actual set\n",
    "    return len(actual_set & predicted_set) / float(len(actual_set))\n",
    "\n",
    "\n",
    "def f1_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate F1@k score.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (set): The set of items that the user actually interacted with or purchased.\n",
    "        predicted (list): The list of items recommended to the user, ranked by relevance.\n",
    "        k (int): The number of top items to consider for the calculation.\n",
    "    \n",
    "    Returns:\n",
    "        float: The F1@k score, which is the harmonic mean of Precision@k and Recall@k.\n",
    "    \"\"\"\n",
    "    # Calculate Precision@k and Recall@k\n",
    "    precision = precision_at_k(actual, predicted, k)\n",
    "    recall = recall_at_k(actual, predicted, k)\n",
    "    \n",
    "    # Handle edge case where both precision and recall are zero to avoid division by zero\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate F1 score as the harmonic mean of precision and recall\n",
    "    return 2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e853edf-ea95-4d43-b959-c58883bee04d",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5052cec-7c01-4e9f-9ac8-383b47f865dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from surprise import Dataset, Reader, SVD, SlopeOne, CoClustering, SVDpp\n",
    "from surprise import accuracy\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "data_dir = '/home/sagemaker-user/Data/'\n",
    "model_dir = '/home/sagemaker-user/Models/'\n",
    "log_dir = '/home/sagemaker-user/Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d0949c-1e65-4e60-a395-8343a417faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(data_dir+'clean_data/train_set.pkl')\n",
    "test_df = pd.read_pickle(data_dir+'clean_data/test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7901ee-5159-4516-b429-10b14590e274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "AG73BVBKUOH22USSFJA5ZWL7AKXA    1426\n",
       "AHPOHKN4PU4W3V5PGFL7AGTAD2AA    1198\n",
       "AHEMJ62SUJPUYNWGROPI6MUAYQ5A     924\n",
       "AEYVPPWR4CIKWX4BGYKCBCDL2CZQ     924\n",
       "AH665SQ6SQF6DXAGYIQFCX76LALA     778\n",
       "                                ... \n",
       "AFI6HA2MBKJJEMGAIBHQOGPO5TMQ       2\n",
       "AFI6G2GDXADHCF5F5NOQ4YGOFLJQ       2\n",
       "AFI6F65FPRQLQZCNY5QGHRHBCAOQ       2\n",
       "AFI6F4Q7RL5DWL7OQIPM5Z6AE2TA       2\n",
       "AFI6JBTMHUXK77EE2DHURX7PFXTQ       2\n",
       "Name: count, Length: 290475, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2411a845-4396-46f6-9ea7-2fa8e8a42c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258132, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3121759b-bc56-48d9-aa71-f6cc80c15bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0002C7FHC</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-09-19 19:42:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B00UFKQKLS</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 15:31:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B01I6X61OQ</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 17:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0713WBZM7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-09-19 19:42:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0BVM3J8GW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-07 15:31:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  label  rating           timestamp\n",
       "0  AE22236AFRRSMQIKGG7TPTB75QEA  B0002C7FHC      1       5 2009-09-19 19:42:10\n",
       "1  AE22236AFRRSMQIKGG7TPTB75QEA  B00UFKQKLS      1       5 2014-03-07 15:31:31\n",
       "2  AE22236AFRRSMQIKGG7TPTB75QEA  B01I6X61OQ      1       5 2014-03-07 17:06:29\n",
       "3  AE22236AFRRSMQIKGG7TPTB75QEA  B0713WBZM7      0       0 2009-09-19 19:42:10\n",
       "4  AE22236AFRRSMQIKGG7TPTB75QEA  B0BVM3J8GW      0       0 2014-03-07 15:31:31"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d449a4-54c6-4d50-8d5a-6fc9a1de8ca8",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7ea25d-0609-402f-8ab9-8e20f9612a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: SVD\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.8507\n",
      "\n",
      "Training model: SVD++\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.8464\n",
      "\n",
      "Training model: CoClustering\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.9400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from surprise import Dataset, Reader, SVD, SVDpp, SlopeOne, CoClustering, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess explicit feedback data for collaborative filtering\n",
    "# Define rating range for explicit feedback\n",
    "reader_explicit = Reader(rating_scale=(1, 5))  # Explicit ratings range from 1 to 5\n",
    "\n",
    "# Load training data\n",
    "# train_df should be a DataFrame containing 'user_id', 'parent_asin' (item ID), and 'rating' columns\n",
    "train_explicit = Dataset.load_from_df(train_df[['user_id', 'parent_asin', 'rating']], reader_explicit)\n",
    "trainset_explicit = train_explicit.build_full_trainset()  # Convert to training set format required by Surprise library\n",
    "\n",
    "# Load testing data\n",
    "# test_df should contain 'user_id', 'parent_asin', and 'rating' (or 'label' for classification)\n",
    "test_set_explicit = list(zip(test_df['user_id'], test_df['parent_asin'], test_df['rating']))\n",
    "\n",
    "# Define models for explicit feedback prediction\n",
    "# Removed NMF model as per the requirement\n",
    "models_explicit = {\n",
    "    'SVD': SVD(random_state=seed),  # Singular Value Decomposition\n",
    "    'SVD++': SVDpp(random_state=seed),  # Enhanced version of SVD considering implicit feedback\n",
    "    # 'SlopeOne': SlopeOne(),  # Simple and efficient collaborative filtering algorithm\n",
    "    'CoClustering': CoClustering(random_state=seed)  # Co-clustering approach for collaborative filtering\n",
    "}\n",
    "\n",
    "# Train each model and evaluate predictions\n",
    "model_predictions_explicit = {}  # Store predictions for each model\n",
    "model_metrics_explicit = {}  # Store evaluation metrics for each model\n",
    "\n",
    "for model_name, model in models_explicit.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    model.fit(trainset_explicit)  # Train the model on the training set\n",
    "\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    predictions = model.test(test_set_explicit)  # Generate predictions on the test set\n",
    "\n",
    "    # Calculate RMSE (Root Mean Square Error) for prediction quality\n",
    "    rmse = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "    # Store predictions\n",
    "    model_predictions_explicit[model_name] = predictions\n",
    "\n",
    "    # Calculate ROC AUC (Area Under the Receiver Operating Characteristic Curve)\n",
    "    y_true = test_df['label']  # True binary labels for the test set\n",
    "    y_pred = [pred.est for pred in predictions]  # Predicted ratings\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Store evaluation metrics\n",
    "    model_metrics_explicit[model_name] = {\n",
    "        'RMSE': rmse,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d323d2dc-8abb-45cc-aa04-c5099f84c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from surprise import Dataset, Reader, SVD, SVDpp, SlopeOne, CoClustering, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess explicit feedback data for collaborative filtering\n",
    "# Define rating range for explicit feedback\n",
    "reader_explicit = Reader(rating_scale=(1, 5))  # Explicit ratings range from 1 to 5\n",
    "\n",
    "# Load training data\n",
    "# train_df should be a DataFrame containing 'user_id', 'parent_asin' (item ID), and 'rating' columns\n",
    "train_explicit = Dataset.load_from_df(train_df[['user_id', 'parent_asin', 'rating']], reader_explicit)\n",
    "trainset_explicit = train_explicit.build_full_trainset()  # Convert to training set format required by Surprise library\n",
    "\n",
    "# Load testing data\n",
    "# test_df should contain 'user_id', 'parent_asin', and 'rating' (or 'label' for classification)\n",
    "test_set_explicit = list(zip(test_df['user_id'], test_df['parent_asin'], test_df['rating']))\n",
    "\n",
    "# Define models for explicit feedback prediction\n",
    "# Removed NMF model as per the requirement\n",
    "models_explicit = {\n",
    "    'SVD': SVD(random_state=seed),  # Singular Value Decomposition\n",
    "    'SVD++': SVDpp(random_state=seed),  # Enhanced version of SVD considering implicit feedback\n",
    "    # 'SlopeOne': SlopeOne(),  # Simple and efficient collaborative filtering algorithm\n",
    "    'CoClustering': CoClustering(random_state=seed)  # Co-clustering approach for collaborative filtering\n",
    "}\n",
    "\n",
    "# Train each model and evaluate predictions\n",
    "model_predictions_explicit = {}  # Store predictions for each model\n",
    "model_metrics_explicit = {}  # Store evaluation metrics for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976f7ac0-9120-4199-8c84-a8d1e649376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: SVD\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.8507\n",
      "\n",
      "Training model: SVD++\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.8464\n",
      "\n",
      "Training model: CoClustering\n",
      "Making predictions on the test set...\n",
      "RMSE: 1.9400\n",
      "Evaluation metrics have been saved to ../Results/CF_model/metrics.txt\n",
      "Predictions for the SVD model have been saved to ../Results/CF_model/SVD_predictions.txt\n",
      "Predictions for the SVD++ model have been saved to ../Results/CF_model/SVD++_predictions.txt\n",
      "Predictions for the CoClustering model have been saved to ../Results/CF_model/CoClustering_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models_explicit.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    model.fit(trainset_explicit)  # Train the model on the training set\n",
    "\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    predictions = model.test(test_set_explicit)  # Generate predictions on the test set\n",
    "\n",
    "    # Calculate RMSE (Root Mean Square Error) for prediction quality\n",
    "    rmse = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "    # Store predictions\n",
    "    model_predictions_explicit[model_name] = predictions\n",
    "\n",
    "    # Calculate ROC AUC (Area Under the Receiver Operating Characteristic Curve)\n",
    "    y_true = test_df['label']  # True binary labels for the test set\n",
    "    y_pred = [pred.est for pred in predictions]  # Predicted ratings\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Store evaluation metrics\n",
    "    model_metrics_explicit[model_name] = {\n",
    "        'RMSE': rmse,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# Define the directory for saving results\n",
    "result_dir = os.path.join('../Results', 'CF_model')  # Update path to reflect 'collaborative_filtering'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# Save evaluation metrics to a text file\n",
    "metrics_file_path = os.path.join(result_dir, 'metrics.txt')\n",
    "\n",
    "with open(metrics_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Evaluation Metrics for Explicit Feedback Models\\n\")\n",
    "    f.write(\"=============================================\\n\\n\")\n",
    "    for model_name, metrics in model_metrics_explicit.items():\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"RMSE: {metrics['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {metrics['ROC_AUC']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Evaluation metrics have been saved to {metrics_file_path}\")\n",
    "\n",
    "# Save predictions for each model to separate files\n",
    "for model_name, predictions in model_predictions_explicit.items():\n",
    "    # Define the file path for predictions\n",
    "    predictions_file_path = os.path.join(result_dir, f\"{model_name}_predictions.txt\")\n",
    "    \n",
    "    # Write predictions to the file\n",
    "    with open(predictions_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{model_name} Model Predictions\\n\")\n",
    "        f.write(\"=============================================\\n\\n\")\n",
    "        for pred in predictions:\n",
    "            f.write(f\"User ID: {pred.uid}, Item ID: {pred.iid}, True Rating: {pred.r_ui}, Predicted Rating: {pred.est}\\n\")\n",
    "    \n",
    "    print(f\"Predictions for the {model_name} model have been saved to {predictions_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6116c9a-63e6-4c9e-9b44-83cde74cd468",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d533300-00e3-4c05-a530-388259b69315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations and calculating evaluation metrics for explicit feedback models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793ac44f97cc476cae127db6764381da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing explicit models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2f089bdcb34eefa83350638477a7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating SVD:   0%|          | 0/290493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43de0480a9a40c98bfabf6d615dc604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating SVD++:   0%|          | 0/290493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e566b0258854c4f9bf1279ab311b0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating CoClustering:   0%|          | 0/290493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 10. Generate recommendation lists and compute evaluation metrics\n",
    "topK_values = [5, 10, 15]  # Define the top-K values to evaluate (e.g., top-5, top-10, top-15 recommendations)\n",
    "results_explicit = {\n",
    "    model_name: {k: {'precision': [], 'recall': [], 'f1': []} for k in topK_values}\n",
    "    for model_name in models_explicit.keys()\n",
    "}  # Initialize a dictionary to store evaluation metrics (precision, recall, F1) for each model and K value\n",
    "\n",
    "print(\"Generating recommendations and calculating evaluation metrics for explicit feedback models...\")\n",
    "\n",
    "# Build a dictionary mapping each user to the set of items they actually interacted with or purchased\n",
    "user_actual = (\n",
    "    test_df[test_df['label'] == 1]  # Filter the test data for items labeled as \"purchased\" or \"positive interactions\"\n",
    "    .groupby('user_id')['parent_asin']  # Group by user ID\n",
    "    .apply(set)  # Convert each user's items to a set\n",
    "    .to_dict()  # Convert the grouped object to a dictionary\n",
    ")\n",
    "\n",
    "# Get the list of all unique users in the test dataset\n",
    "all_users = test_df['user_id'].unique()\n",
    "\n",
    "# Process predictions for each model\n",
    "for model_name, predictions in tqdm(model_predictions_explicit.items(), desc=\"Processing explicit models\"):\n",
    "    user_recommendations = defaultdict(list)  # Initialize a dictionary to store recommendations for each user\n",
    "    \n",
    "    # Group predictions by user\n",
    "    user_pred_dict = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        # Each prediction contains a user ID (uid), an item ID (iid), and an estimated rating (est)\n",
    "        user_pred_dict[pred.uid].append((pred.iid, pred.est))\n",
    "    \n",
    "    # Generate a sorted recommendation list for each user\n",
    "    for user, item_scores in user_pred_dict.items():\n",
    "        # Sort items by estimated score in descending order\n",
    "        sorted_items = sorted(item_scores, key=lambda x: x[1], reverse=True)\n",
    "        # Extract only the item IDs from the sorted list\n",
    "        recommended_items = [item for item, score in sorted_items]\n",
    "        # Store the sorted recommendations for the user\n",
    "        user_recommendations[user] = recommended_items\n",
    "    \n",
    "    # Evaluate the recommendations for each user\n",
    "    for user in tqdm(all_users, desc=f\"Evaluating {model_name}\"):\n",
    "        actual = user_actual.get(user, set())  # Get the set of items the user actually interacted with\n",
    "        predicted = user_recommendations.get(user, [])  # Get the list of recommended items for the user\n",
    "        \n",
    "        # Calculate evaluation metrics for each K value\n",
    "        for k in topK_values:\n",
    "            precision = precision_at_k(actual, predicted, k)  # Calculate Precision@k\n",
    "            recall = recall_at_k(actual, predicted, k)  # Calculate Recall@k\n",
    "            f1 = f1_at_k(actual, predicted, k)  # Calculate F1@k\n",
    "            \n",
    "            # Append the metrics to the results dictionary\n",
    "            results_explicit[model_name][k]['precision'].append(precision)\n",
    "            results_explicit[model_name][k]['recall'].append(recall)\n",
    "            results_explicit[model_name][k]['f1'].append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b4bed5-1457-41f2-84b3-095afd89e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: SVD\n",
      "\n",
      "Evaluation metrics for top-5:\n",
      "Precision@5: 0.8018\n",
      "Recall@5: 0.7227\n",
      "F1-score@5: 0.7365\n",
      "\n",
      "Evaluation metrics for top-10:\n",
      "Precision@10: 0.5535\n",
      "Recall@10: 0.9404\n",
      "F1-score@10: 0.6755\n",
      "\n",
      "Evaluation metrics for top-15:\n",
      "Precision@15: 0.3960\n",
      "Recall@15: 0.9870\n",
      "F1-score@15: 0.5497\n",
      "RMSE: 1.8507\n",
      "ROC AUC: 0.8627\n",
      "\n",
      "Model: SVD++\n",
      "\n",
      "Evaluation metrics for top-5:\n",
      "Precision@5: 0.8000\n",
      "Recall@5: 0.7223\n",
      "F1-score@5: 0.7355\n",
      "\n",
      "Evaluation metrics for top-10:\n",
      "Precision@10: 0.5548\n",
      "Recall@10: 0.9422\n",
      "F1-score@10: 0.6770\n",
      "\n",
      "Evaluation metrics for top-15:\n",
      "Precision@15: 0.3981\n",
      "Recall@15: 0.9903\n",
      "F1-score@15: 0.5523\n",
      "RMSE: 1.8464\n",
      "ROC AUC: 0.8598\n",
      "\n",
      "Model: CoClustering\n",
      "\n",
      "Evaluation metrics for top-5:\n",
      "Precision@5: 0.7894\n",
      "Recall@5: 0.7189\n",
      "F1-score@5: 0.7292\n",
      "\n",
      "Evaluation metrics for top-10:\n",
      "Precision@10: 0.5563\n",
      "Recall@10: 0.9457\n",
      "F1-score@10: 0.6793\n",
      "\n",
      "Evaluation metrics for top-15:\n",
      "Precision@15: 0.4006\n",
      "Recall@15: 0.9942\n",
      "F1-score@15: 0.5554\n",
      "RMSE: 1.9400\n",
      "ROC AUC: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# 11. Print evaluation results for each model\n",
    "for model_name in models_explicit.keys():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for k in topK_values:\n",
    "        # Calculate average precision, recall, and F1-score for top-K recommendations\n",
    "        avg_precision = np.mean(results_explicit[model_name][k]['precision'])\n",
    "        avg_recall = np.mean(results_explicit[model_name][k]['recall'])\n",
    "        avg_f1 = np.mean(results_explicit[model_name][k]['f1'])\n",
    "        \n",
    "        # Print evaluation metrics for the current top-K value\n",
    "        print(f\"\\nEvaluation metrics for top-{k}:\")\n",
    "        print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "        print(f\"F1-score@{k}: {avg_f1:.4f}\")\n",
    "    \n",
    "    # Print RMSE and ROC AUC for the model\n",
    "    print(f\"RMSE: {model_metrics_explicit[model_name]['RMSE']:.4f}\")\n",
    "    print(f\"ROC AUC: {model_metrics_explicit[model_name]['ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c328012-4440-484d-b812-c66e27975713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics have been saved to ../Results/CF_model/metrics_classify.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save evaluation metrics\n",
    "metrics_file_path = os.path.join(result_dir, 'metrics_classify.txt')\n",
    "\n",
    "# Write evaluation metrics to the file\n",
    "with open(metrics_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Evaluation Metrics for Explicit Feedback Models\\n\")\n",
    "    f.write(\"========================\\n\\n\")\n",
    "    \n",
    "    # Iterate through each model and save its metrics\n",
    "    for model_name in models_explicit.keys():\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        \n",
    "        # Write evaluation metrics for each top-K value\n",
    "        for k in topK_values:\n",
    "            avg_precision = np.mean(results_explicit[model_name][k]['precision'])  # Average Precision@k\n",
    "            avg_recall = np.mean(results_explicit[model_name][k]['recall'])  # Average Recall@k\n",
    "            avg_f1 = np.mean(results_explicit[model_name][k]['f1'])  # Average F1@k\n",
    "            \n",
    "            f.write(f\"\\nEvaluation Metrics for top-{k}:\\n\")\n",
    "            f.write(f\"Precision@{k}: {avg_precision:.4f}\\n\")\n",
    "            f.write(f\"Recall@{k}: {avg_recall:.4f}\\n\")\n",
    "            f.write(f\"F1-score@{k}: {avg_f1:.4f}\\n\")\n",
    "        \n",
    "        # Write global metrics: RMSE and ROC AUC\n",
    "        f.write(f\"RMSE: {model_metrics_explicit[model_name]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {model_metrics_explicit[model_name]['ROC_AUC']:.4f}\\n\")\n",
    "        f.write(\"\\n------------------------\\n\\n\")\n",
    "\n",
    "# Notify the user that the metrics have been saved\n",
    "print(f\"Evaluation metrics have been saved to {metrics_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbeae6e7-7691-4d16-87fa-7aa255c9c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SVD++, RMSE: 1.8464\n",
      "The best model has been saved to ../Results/CF_model/SVD++_best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory to save results\n",
    "result_dir = os.path.join('../Results', 'CF_model')\n",
    "os.makedirs(result_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Identify the best model based on RMSE\n",
    "best_model_name = min(model_metrics_explicit, key=lambda x: model_metrics_explicit[x]['RMSE'])  # Select model with lowest RMSE\n",
    "best_rmse = model_metrics_explicit[best_model_name]['RMSE']\n",
    "print(f\"Best model: {best_model_name}, RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Save the best model to a file\n",
    "model_file_path = os.path.join(result_dir, f\"{best_model_name}_best_model.pkl\")\n",
    "with open(model_file_path, 'wb') as f:\n",
    "    pickle.dump(models_explicit[best_model_name], f)  # Serialize and save the best model\n",
    "print(f\"The best model has been saved to {model_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0800f-dc97-4cb0-82ce-857b811843d8",
   "metadata": {},
   "source": [
    "## Latent Factors and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8993c9-ce90-413c-8e9b-294f9493a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User and item vectors and mappings have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Extract user and item latent factor matrices\n",
    "user_factors = models_explicit[best_model_name].pu  # User factor matrix: shape (n_users, n_factors)\n",
    "item_factors = models_explicit[best_model_name].qi  # Item factor matrix: shape (n_items, n_factors)\n",
    "\n",
    "# Retrieve user and item ID mappings\n",
    "trainset = models_explicit[best_model_name].trainset  # Access the trainset used by the model\n",
    "user_id_map = {trainset.to_raw_uid(i): i for i in range(trainset.n_users)}  # Map raw user IDs to internal IDs\n",
    "item_id_map = {trainset.to_raw_iid(i): i for i in range(trainset.n_items)}  # Map raw item IDs to internal IDs\n",
    "\n",
    "# Save user factors and ID mapping\n",
    "np.save(os.path.join(result_dir, 'user_factors.npy'), user_factors)  # Save user latent factors as a NumPy array\n",
    "with open(os.path.join(result_dir, 'user_id_map.pkl'), 'wb') as f:\n",
    "    pickle.dump(user_id_map, f)  # Save user ID mapping as a pickle file\n",
    "\n",
    "# Save item factors and ID mapping\n",
    "np.save(os.path.join(result_dir, 'item_factors.npy'), item_factors)  # Save item latent factors as a NumPy array\n",
    "with open(os.path.join(result_dir, 'item_id_map.pkl'), 'wb') as f:\n",
    "    pickle.dump(item_id_map, f)  # Save item ID mapping as a pickle file\n",
    "\n",
    "print(\"User and item vectors and mappings have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f963528-aa35-4428-9c83-93bbbb97686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index...\n",
      "FAISS vector index has been saved as 'item_index.faiss'.\n",
      "FAISS index successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index for item vectors\n",
    "print(\"Creating FAISS index...\")\n",
    "item_factors_normalized = normalize(item_factors, norm='l2').astype('float32')  # Normalize item factors for cosine similarity\n",
    "d = item_factors_normalized.shape[1]  # Dimensionality of the vectors\n",
    "index = faiss.IndexFlatIP(d)  # Create a FAISS index for inner product similarity\n",
    "index.add(item_factors_normalized)  # Add normalized item factors to the index\n",
    "faiss.write_index(index, os.path.join(result_dir, 'item_index.faiss'))  # Save the index to a file\n",
    "print(\"FAISS vector index has been saved as 'item_index.faiss'.\")\n",
    "\n",
    "# Load the saved model and latent factor vectors\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    best_model = pickle.load(f)  # Load the best model\n",
    "\n",
    "user_factors = np.load(os.path.join(result_dir, 'user_factors.npy')).astype('float32')  # Load user factors\n",
    "item_factors = np.load(os.path.join(result_dir, 'item_factors.npy')).astype('float32')  # Load item factors\n",
    "\n",
    "# Load user and item ID mappings\n",
    "with open(os.path.join(result_dir, 'user_id_map.pkl'), 'rb') as f:\n",
    "    user_id_map = pickle.load(f)  # Load user ID mapping\n",
    "with open(os.path.join(result_dir, 'item_id_map.pkl'), 'rb') as f:\n",
    "    item_id_map = pickle.load(f)  # Load item ID mapping\n",
    "\n",
    "# Create reverse mappings for user and item IDs\n",
    "item_id_inverse_mapping = {v: k for k, v in item_id_map.items()}  # Reverse mapping for item IDs\n",
    "user_id_inverse_mapping = {v: k for k, v in user_id_map.items()}  # Reverse mapping for user IDs\n",
    "\n",
    "# Load the saved FAISS index\n",
    "index = faiss.read_index(os.path.join(result_dir, 'item_index.faiss'))  # Load the FAISS index from file\n",
    "print(\"FAISS index successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8ae0a-36a7-468f-a981-9fda43f4b699",
   "metadata": {},
   "source": [
    "## Batch Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430b3f97-d035-48c8-8264-496e93946a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendation lists using FAISS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b2e2e904474b52a14bc337eeda8a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating recommendations:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation list generation completed.\n"
     ]
    }
   ],
   "source": [
    "# Normalize user factors for cosine similarity\n",
    "faiss.normalize_L2(user_factors)\n",
    "\n",
    "# Define recommendation parameters\n",
    "topK = 20  # Number of items to recommend per user\n",
    "batch_size = 1000  # Number of users processed in each batch\n",
    "n_users = user_factors.shape[0]  # Total number of users\n",
    "all_recommendations = {}  # Dictionary to store recommendations for all users\n",
    "\n",
    "# Load actual items purchased by each user\n",
    "user_actual = (\n",
    "    test_df[test_df['label'] == 1]  # Filter positive interactions (purchased items)\n",
    "    .groupby('user_id')['parent_asin']  # Group by user ID and collect item IDs\n",
    "    .apply(set)  # Convert item lists to sets for each user\n",
    "    .to_dict()  # Convert the grouped object to a dictionary\n",
    ")\n",
    "\n",
    "print(\"Generating recommendation lists using FAISS...\")\n",
    "\n",
    "# Batch processing to generate recommendations for all users\n",
    "for start in tqdm(range(0, n_users, batch_size), desc=\"Generating recommendations\"):\n",
    "    end = min(start + batch_size, n_users)  # Define batch range\n",
    "    batch_user_factors = user_factors[start:end]  # Extract user factors for the batch\n",
    "    \n",
    "    # Search for top-K similar items for the current batch of users\n",
    "    D, I = index.search(batch_user_factors, topK)  # D: Similarity scores, I: Indices of recommended items\n",
    "    \n",
    "    # Process recommendations for each user in the batch\n",
    "    for i in range(end - start):\n",
    "        user_idx = start + i  # Calculate the global user index\n",
    "        recommended_item_indices = I[i]  # Get recommended item indices for the user\n",
    "        # Convert internal item indices to raw item IDs using trainset mapping\n",
    "        recommended_items = [best_model.trainset.to_raw_iid(item_idx) for item_idx in recommended_item_indices]\n",
    "        all_recommendations[user_idx] = recommended_items  # Store recommendations for the user\n",
    "\n",
    "print(\"Recommendation list generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c353cb-1167-4395-bf97-3e32cbc0ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample recommendation results:\n",
      "User ID: AE22236AFRRSMQIKGG7TPTB75QEA, Number of recommendations: 20, Recommendations: ['B007NFLN1K', 'B005HSPOVU', 'B01KZTB3HE', 'B0BS72KD59', 'B00E3UKVVQ', 'B07FLSK36C', 'B07S3RFL7V', 'B01FZIWFLS', 'B09FPR3626', 'B003194PBC', 'B09PRVBLTL', 'B000W7IR10', 'B071489L22', 'B01K4KU5HI', 'B07DN8GCLG', 'B00T62YNQK', 'B0002568EK', 'B081RM74FQ', 'B07KF5G7XP', 'B00O4DKHRK']\n",
      "User ID: AE222MW56PH6JXPIB6XSAMCBTLNQ, Number of recommendations: 20, Recommendations: ['B0002ASBYU', 'B0BL9SWZH5', 'B007ZTL4LS', 'B06Y3RQB1N', 'B07NLZTHD7', 'B0096JVJWO', 'B014854DDM', 'B0BDZPM7SQ', 'B00SG7O9Q0', 'B001CS2MKA', 'B019BJEIRQ', 'B00G9YL8XY', 'B089JHTRMM', 'B0009XPD64', 'B09XJV967L', 'B005OQ35RK', 'B0BC2RFPPR', 'B008EVOTVI', 'B003IWGYF2', 'B01N6YKXZU']\n",
      "User ID: AE222N3VUKMF3GO6D4LHTELE7UWA, Number of recommendations: 20, Recommendations: ['B06XFRPM63', 'B006684SEE', 'B004PBIJ4O', 'B01A5VEPTA', 'B00DM0OBNM', 'B008XKX3BC', 'B07M683JK4', 'B07CNC8HR8', 'B01H1GDWT6', 'B000HHLRJO', 'B076JB3C54', 'B082WRFNGH', 'B01IO63PL2', 'B0077RDP9M', 'B08CKH3RVT', 'B00FF2Q36I', 'B01KM2IYRG', 'B08SVRVLGB', 'B08DFYVXV6', 'B002DZNQEW']\n",
      "User ID: AE2244ILMBLRPTIN7VW7YDKRI2YA, Number of recommendations: 20, Recommendations: ['B00BUFTCW6', 'B00WIRW506', 'B00OC213DA', 'B011QCYFJ2', 'B01488549Q', 'B008GFY9QC', 'B0BXCJ1HTF', 'B00KM9CJOE', 'B07TRPDHRY', 'B00E0ETUGW', 'B00HWQONLM', 'B07HNFNC5G', 'B00798FYR0', 'B00CSQWR82', 'B0002563UY', 'B00ALNUBWS', 'B000Y91FRY', 'B006GHXZ7Q', 'B008XFOO38', 'B01D53L1A6']\n",
      "User ID: AE226BJM6RTWIVV6UJKZAVQPBKXA, Number of recommendations: 20, Recommendations: ['B000G18EHE', 'B08D8HVTPG', 'B07SCFW71M', 'B00BK3RR48', 'B001VPCDXK', 'B002EVKUQM', 'B01M26VFQN', 'B06XQ65XTR', 'B09GKL8KLF', 'B08B3LKRMQ', 'B00D3426WU', 'B07F68XRSJ', 'B000255NI2', 'B00HY1OYIC', 'B00GUG1AQ6', 'B0002GLYFM', 'B07MDMV963', 'B07C512HCG', 'B07K97XJLF', 'B013OLDX2I']\n",
      "Recommendations saved to ../Results/CF_model/recommendations.h5\n",
      "Recommendations successfully loaded.\n",
      "\n",
      "Sample loaded recommendation results:\n",
      "User ID: AE22236AFRRSMQIKGG7TPTB75QEA, Number of recommendations: 20, Recommendations: ['B007NFLN1K', 'B005HSPOVU', 'B01KZTB3HE', 'B0BS72KD59', 'B00E3UKVVQ', 'B07FLSK36C', 'B07S3RFL7V', 'B01FZIWFLS', 'B09FPR3626', 'B003194PBC', 'B09PRVBLTL', 'B000W7IR10', 'B071489L22', 'B01K4KU5HI', 'B07DN8GCLG', 'B00T62YNQK', 'B0002568EK', 'B081RM74FQ', 'B07KF5G7XP', 'B00O4DKHRK']\n",
      "User ID: AE222MW56PH6JXPIB6XSAMCBTLNQ, Number of recommendations: 20, Recommendations: ['B0002ASBYU', 'B0BL9SWZH5', 'B007ZTL4LS', 'B06Y3RQB1N', 'B07NLZTHD7', 'B0096JVJWO', 'B014854DDM', 'B0BDZPM7SQ', 'B00SG7O9Q0', 'B001CS2MKA', 'B019BJEIRQ', 'B00G9YL8XY', 'B089JHTRMM', 'B0009XPD64', 'B09XJV967L', 'B005OQ35RK', 'B0BC2RFPPR', 'B008EVOTVI', 'B003IWGYF2', 'B01N6YKXZU']\n",
      "User ID: AE222N3VUKMF3GO6D4LHTELE7UWA, Number of recommendations: 20, Recommendations: ['B06XFRPM63', 'B006684SEE', 'B004PBIJ4O', 'B01A5VEPTA', 'B00DM0OBNM', 'B008XKX3BC', 'B07M683JK4', 'B07CNC8HR8', 'B01H1GDWT6', 'B000HHLRJO', 'B076JB3C54', 'B082WRFNGH', 'B01IO63PL2', 'B0077RDP9M', 'B08CKH3RVT', 'B00FF2Q36I', 'B01KM2IYRG', 'B08SVRVLGB', 'B08DFYVXV6', 'B002DZNQEW']\n",
      "User ID: AE2244ILMBLRPTIN7VW7YDKRI2YA, Number of recommendations: 20, Recommendations: ['B00BUFTCW6', 'B00WIRW506', 'B00OC213DA', 'B011QCYFJ2', 'B01488549Q', 'B008GFY9QC', 'B0BXCJ1HTF', 'B00KM9CJOE', 'B07TRPDHRY', 'B00E0ETUGW', 'B00HWQONLM', 'B07HNFNC5G', 'B00798FYR0', 'B00CSQWR82', 'B0002563UY', 'B00ALNUBWS', 'B000Y91FRY', 'B006GHXZ7Q', 'B008XFOO38', 'B01D53L1A6']\n",
      "User ID: AE226BJM6RTWIVV6UJKZAVQPBKXA, Number of recommendations: 20, Recommendations: ['B000G18EHE', 'B08D8HVTPG', 'B07SCFW71M', 'B00BK3RR48', 'B001VPCDXK', 'B002EVKUQM', 'B01M26VFQN', 'B06XQ65XTR', 'B09GKL8KLF', 'B08B3LKRMQ', 'B00D3426WU', 'B07F68XRSJ', 'B000255NI2', 'B00HY1OYIC', 'B00GUG1AQ6', 'B0002GLYFM', 'B07MDMV963', 'B07C512HCG', 'B07K97XJLF', 'B013OLDX2I']\n"
     ]
    }
   ],
   "source": [
    "# Map recommendations back to raw user and item IDs, and filter out already-seen items\n",
    "user_recommendations_raw = {}\n",
    "\n",
    "for user_idx, recommended_item_ids in all_recommendations.items():\n",
    "    user_id = user_id_inverse_mapping.get(user_idx, None)  # Map internal user ID to raw user ID\n",
    "    if user_id is not None:\n",
    "        # Get items the user has already interacted with (raw IDs)\n",
    "        seen_items = user_actual.get(user_id, set())\n",
    "        # Filter out items already seen by the user\n",
    "        recommended_item_ids_filtered = [item_id for item_id in recommended_item_ids if item_id not in seen_items]\n",
    "        # Keep only the top-K recommendations\n",
    "        recommended_item_ids_filtered = recommended_item_ids_filtered[:topK]\n",
    "        user_recommendations_raw[user_id] = recommended_item_ids_filtered  # Store the filtered recommendations\n",
    "\n",
    "# Check example recommendations\n",
    "print(\"\\nSample recommendation results:\")\n",
    "for user_id, recs in list(user_recommendations_raw.items())[:5]:\n",
    "    print(f\"User ID: {user_id}, Number of recommendations: {len(recs)}, Recommendations: {recs}\")\n",
    "\n",
    "# Save recommendations to an HDF5 file\n",
    "with h5py.File(os.path.join(result_dir, 'recommendations.h5'), 'w') as hf:\n",
    "    for user_id, recommended_items in user_recommendations_raw.items():\n",
    "        # Pad the recommendation list to ensure uniform length (e.g., top-K)\n",
    "        recommended_items_padded = recommended_items + [''] * (topK - len(recommended_items))\n",
    "        hf.create_dataset(user_id, data=np.string_(recommended_items_padded))  # Save as fixed-length strings\n",
    "\n",
    "print(f\"Recommendations saved to {os.path.join(result_dir, 'recommendations.h5')}\")\n",
    "\n",
    "# Load recommendations from the HDF5 file\n",
    "loaded_recommendations = {}\n",
    "with h5py.File(os.path.join(result_dir, 'recommendations.h5'), 'r') as hf:\n",
    "    for user_id in hf.keys():\n",
    "        recommended_items = hf[user_id][:]  # Load the recommendation list\n",
    "        # Convert bytes to strings and remove padding\n",
    "        recommended_items = [item.decode('utf-8') for item in recommended_items if item.decode('utf-8')]\n",
    "        loaded_recommendations[user_id] = recommended_items  # Store the recommendations\n",
    "\n",
    "print(\"Recommendations successfully loaded.\")\n",
    "\n",
    "# Check loaded recommendations\n",
    "print(\"\\nSample loaded recommendation results:\")\n",
    "for user_id, recs in list(loaded_recommendations.items())[:5]:\n",
    "    print(f\"User ID: {user_id}, Number of recommendations: {len(recs)}, Recommendations: {recs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8626caec-82d4-4595-8621-7d109278dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendation list for user AE22236AFRRSMQIKGG7TPTB75QEA:\n",
      "1. B007NFLN1K\n",
      "2. B005HSPOVU\n",
      "3. B01KZTB3HE\n",
      "4. B0BS72KD59\n",
      "5. B00E3UKVVQ\n",
      "6. B07FLSK36C\n",
      "7. B07S3RFL7V\n",
      "8. B01FZIWFLS\n",
      "9. B09FPR3626\n",
      "10. B003194PBC\n",
      "11. B09PRVBLTL\n",
      "12. B000W7IR10\n",
      "13. B071489L22\n",
      "14. B01K4KU5HI\n",
      "15. B07DN8GCLG\n",
      "16. B00T62YNQK\n",
      "17. B0002568EK\n",
      "18. B081RM74FQ\n",
      "19. B07KF5G7XP\n",
      "20. B00O4DKHRK\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get recommendations for a single user\n",
    "def get_user_recommendations(user_id: str, loaded_recommendations: dict) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve the recommendation list for a single user.\n",
    "    \n",
    "    Parameters:\n",
    "        user_id (str): Unique identifier for the user.\n",
    "        loaded_recommendations (dict): Dictionary containing loaded recommendation results.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of recommended items for the user. Returns an empty list if the user is not found.\n",
    "    \"\"\"\n",
    "    return loaded_recommendations.get(user_id, [])  # Return the recommendations or an empty list if user not found\n",
    "\n",
    "# Test recommendations for a single user\n",
    "example_user_id = 'AE22236AFRRSMQIKGG7TPTB75QEA'  # Replace with the user ID you want to test\n",
    "recommended_items = get_user_recommendations(example_user_id, loaded_recommendations)\n",
    "\n",
    "# Display the user's recommendations\n",
    "if recommended_items:\n",
    "    print(f\"\\nRecommendation list for user {example_user_id}:\")\n",
    "    for rank, item in enumerate(recommended_items, start=1):  # Enumerate recommendations with rankings\n",
    "        print(f\"{rank}. {item}\")\n",
    "else:\n",
    "    print(f\"\\nNo recommendations found for user {example_user_id} or user does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
