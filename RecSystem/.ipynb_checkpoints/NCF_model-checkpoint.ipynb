{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0946c79d-dd6f-4b94-8f72-a392cce453e7",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f5db9d-a98a-442a-8971-8ea6c6dbd92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "data_dir = '/home/sagemaker-user/Data/'\n",
    "model_dir = '/home/sagemaker-user/Models/'\n",
    "log_dir = '/home/sagemaker-user/Logs/'\n",
    "\n",
    "train_df = pd.read_pickle(data_dir+'clean_data/train_set.pkl')\n",
    "test_df = pd.read_pickle(data_dir+'clean_data/test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a6e60e-ba00-4ee8-98ae-17ee1f64eb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data = pd.read_pickle(data_dir+'clean_data/filtered_data.pkl')\n",
    "filtered_data=filtered_data[['parent_asin','text']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcec1d6-1460-436e-8e07-724766cbc901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0002C7FHC</td>\n",
       "      <td>dog whole lot say basic bark control collar ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00UFKQKLS</td>\n",
       "      <td>hamilton nylon comfort dog harness come vibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B01I6X61OQ</td>\n",
       "      <td>high quality adjustable collar quickrelease bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B09C8C44PQ</td>\n",
       "      <td>petnation portacrate portable easy carry take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B001150X9G</td>\n",
       "      <td>poochiebells leader dog potty training communi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin                                               text\n",
       "0  B0002C7FHC  dog whole lot say basic bark control collar ef...\n",
       "1  B00UFKQKLS  hamilton nylon comfort dog harness come vibran...\n",
       "2  B01I6X61OQ  high quality adjustable collar quickrelease bu...\n",
       "3  B09C8C44PQ  petnation portacrate portable easy carry take ...\n",
       "4  B001150X9G  poochiebells leader dog potty training communi..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50ceb8e-c775-42a2-9151-6d2cab09f1be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging:\n",
      "train_df columns: ['user_id', 'parent_asin', 'label', 'rating', 'timestamp']\n",
      "test_df columns: ['user_id', 'parent_asin', 'label', 'rating', 'timestamp']\n",
      "Processing train_df in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████| 8/8 [00:07<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_df in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After merging:\n",
      "train_df_enriched columns: ['user_id', 'parent_asin', 'label', 'rating', 'timestamp', 'text']\n",
      "test_df_enriched columns: ['user_id', 'parent_asin', 'label', 'rating', 'timestamp', 'text']\n",
      "\n",
      "Sample of enriched train_df:\n",
      "                        user_id parent_asin  label  rating  \\\n",
      "0  AE22236AFRRSMQIKGG7TPTB75QEA  B0002C7FHC      1       5   \n",
      "1  AE22236AFRRSMQIKGG7TPTB75QEA  B00UFKQKLS      1       5   \n",
      "2  AE22236AFRRSMQIKGG7TPTB75QEA  B01I6X61OQ      1       5   \n",
      "3  AE22236AFRRSMQIKGG7TPTB75QEA  B0713WBZM7      0       0   \n",
      "4  AE22236AFRRSMQIKGG7TPTB75QEA  B0BVM3J8GW      0       0   \n",
      "\n",
      "            timestamp                                               text  \n",
      "0 2009-09-19 19:42:10  dog whole lot say basic bark control collar ef...  \n",
      "1 2014-03-07 15:31:31  hamilton nylon comfort dog harness come vibran...  \n",
      "2 2014-03-07 17:06:29  high quality adjustable collar quickrelease bu...  \n",
      "3 2009-09-19 19:42:10                                                NaN  \n",
      "4 2014-03-07 15:31:31                                                NaN  \n",
      "\n",
      "Sample of enriched test_df:\n",
      "                        user_id parent_asin  label  rating  \\\n",
      "0  AE22236AFRRSMQIKGG7TPTB75QEA  B09C8C44PQ      1       5   \n",
      "1  AE22236AFRRSMQIKGG7TPTB75QEA  B001150X9G      1       3   \n",
      "2  AE22236AFRRSMQIKGG7TPTB75QEA  B0BTSFVLZ2      1       5   \n",
      "3  AE22236AFRRSMQIKGG7TPTB75QEA  B00LVUOYSW      1       5   \n",
      "4  AE22236AFRRSMQIKGG7TPTB75QEA  B08CGZVL2L      1       5   \n",
      "\n",
      "            timestamp                                               text  \n",
      "0 2014-03-07 17:22:05  petnation portacrate portable easy carry take ...  \n",
      "1 2014-03-07 17:28:54  poochiebells leader dog potty training communi...  \n",
      "2 2014-03-25 13:07:08  little dog provide lot love give puppy best st...  \n",
      "3 2015-03-17 16:10:27  petrainer petdr dog training collar assist cur...  \n",
      "4 2015-03-17 16:20:09  iams proactive health smart puppy food veterin...  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count  # For parallel processing\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import numpy as np  # For array splitting\n",
    "import gc  # For garbage collection to manage memory\n",
    "\n",
    "# Select the columns to merge from the filtered data\n",
    "filtered_subset = filtered_data[['parent_asin', 'text']]  # Keep only 'parent_asin' and 'text' columns\n",
    "\n",
    "# Perform garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Display train_df and test_df structure before merging\n",
    "print(\"Before merging:\")\n",
    "print(\"train_df columns:\", train_df.columns.tolist())\n",
    "print(\"test_df columns:\", test_df.columns.tolist())\n",
    "\n",
    "# Define the batch merge function\n",
    "def merge_batch(args):\n",
    "    \"\"\"\n",
    "    Merges a batch of data with the filtered subset on 'parent_asin' column.\n",
    "\n",
    "    Parameters:\n",
    "        args (tuple): A tuple containing the batch DataFrame and its index.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Merged batch DataFrame.\n",
    "    \"\"\"\n",
    "    batch, batch_index = args  # Unpack the batch and its index\n",
    "    # Merge the batch with filtered_subset on 'parent_asin' (left join)\n",
    "    merged_batch = batch.merge(filtered_subset, on=['parent_asin'], how='left')\n",
    "    # Perform garbage collection to release memory immediately after merging\n",
    "    gc.collect()\n",
    "    return merged_batch\n",
    "\n",
    "# Get the number of CPU cores\n",
    "num_cores = cpu_count()\n",
    "# Set the number of processes for parallel processing (adjustable based on memory)\n",
    "num_processes = max(1, num_cores // 2)\n",
    "# Define the number of batches (based on memory availability)\n",
    "num_batches = num_processes * 2\n",
    "\n",
    "# Process train_df in batches\n",
    "print(\"Processing train_df in batches...\")\n",
    "\n",
    "# Split train_df into multiple batches\n",
    "train_batches = np.array_split(train_df, num_batches)\n",
    "\n",
    "# Store results of merged batches\n",
    "train_results = []\n",
    "# Use multiprocessing Pool for parallel processing\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # Process each batch and append results\n",
    "    for result in tqdm(pool.imap(merge_batch, [(batch, idx) for idx, batch in enumerate(train_batches)]), total=num_batches):\n",
    "        train_results.append(result)\n",
    "        # Perform garbage collection after processing each batch\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenate all processed batches into a single DataFrame\n",
    "train_df_enriched = pd.concat(train_results, ignore_index=True)\n",
    "# Clean up memory\n",
    "del train_results, train_batches, train_df\n",
    "gc.collect()\n",
    "\n",
    "# Process test_df in batches\n",
    "print(\"Processing test_df in batches...\")\n",
    "\n",
    "# Split test_df into multiple batches\n",
    "test_batches = np.array_split(test_df, num_batches)\n",
    "\n",
    "# Store results of merged batches\n",
    "test_results = []\n",
    "# Use multiprocessing Pool for parallel processing\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # Process each batch and append results\n",
    "    for result in tqdm(pool.imap(merge_batch, [(batch, idx) for idx, batch in enumerate(test_batches)]), total=num_batches):\n",
    "        test_results.append(result)\n",
    "        # Perform garbage collection after processing each batch\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenate all processed batches into a single DataFrame\n",
    "test_df_enriched = pd.concat(test_results, ignore_index=True)\n",
    "# Clean up memory\n",
    "del test_results, test_batches, test_df\n",
    "gc.collect()\n",
    "\n",
    "# Delete unnecessary variables and perform garbage collection\n",
    "del filtered_subset\n",
    "gc.collect()\n",
    "\n",
    "# Display train_df and test_df structure after merging\n",
    "print(\"\\nAfter merging:\")\n",
    "print(\"train_df_enriched columns:\", train_df_enriched.columns.tolist())\n",
    "print(\"test_df_enriched columns:\", test_df_enriched.columns.tolist())\n",
    "\n",
    "# Display a sample of the enriched train_df\n",
    "print(\"\\nSample of enriched train_df:\")\n",
    "print(train_df_enriched.head())\n",
    "\n",
    "# Display a sample of the enriched test_df\n",
    "print(\"\\nSample of enriched test_df:\")\n",
    "print(test_df_enriched.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd41f72-9c62-4625-a7d6-80437eb1e838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0002C7FHC</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-09-19 19:42:10</td>\n",
       "      <td>dog whole lot say basic bark control collar ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B00UFKQKLS</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 15:31:31</td>\n",
       "      <td>hamilton nylon comfort dog harness come vibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B01I6X61OQ</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 17:06:29</td>\n",
       "      <td>high quality adjustable collar quickrelease bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0713WBZM7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-09-19 19:42:10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0BVM3J8GW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-07 15:31:31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  label  rating  \\\n",
       "0  AE22236AFRRSMQIKGG7TPTB75QEA  B0002C7FHC      1       5   \n",
       "1  AE22236AFRRSMQIKGG7TPTB75QEA  B00UFKQKLS      1       5   \n",
       "2  AE22236AFRRSMQIKGG7TPTB75QEA  B01I6X61OQ      1       5   \n",
       "3  AE22236AFRRSMQIKGG7TPTB75QEA  B0713WBZM7      0       0   \n",
       "4  AE22236AFRRSMQIKGG7TPTB75QEA  B0BVM3J8GW      0       0   \n",
       "\n",
       "            timestamp                                               text  \n",
       "0 2009-09-19 19:42:10  dog whole lot say basic bark control collar ef...  \n",
       "1 2014-03-07 15:31:31  hamilton nylon comfort dog harness come vibran...  \n",
       "2 2014-03-07 17:06:29  high quality adjustable collar quickrelease bu...  \n",
       "3 2009-09-19 19:42:10                                                NaN  \n",
       "4 2014-03-07 15:31:31                                                NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777eae52-0bb5-4de1-b548-187645778364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_df_enriched after dropping missing values: 938741\n",
      "Number of rows in test_df_enriched after dropping missing values: 2657241\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values from the enriched train DataFrame\n",
    "train_df_enriched_cleaned = train_df_enriched.dropna()\n",
    "\n",
    "# Drop rows with missing values from the enriched test DataFrame\n",
    "test_df_enriched_cleaned = test_df_enriched.dropna()\n",
    "\n",
    "# Display the number of rows after removing missing values\n",
    "print(\"Number of rows in train_df_enriched after dropping missing values:\", len(train_df_enriched_cleaned))\n",
    "print(\"Number of rows in test_df_enriched after dropping missing values:\", len(test_df_enriched_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7332edf-7880-4073-97c4-e39f54627dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0002C7FHC</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-09-19 19:42:10</td>\n",
       "      <td>dog whole lot say basic bark control collar ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B00UFKQKLS</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 15:31:31</td>\n",
       "      <td>hamilton nylon comfort dog harness come vibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B01I6X61OQ</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-07 17:06:29</td>\n",
       "      <td>high quality adjustable collar quickrelease bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AE22236AFRRSMQIKGG7TPTB75QEA</td>\n",
       "      <td>B0085JN2JY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-07 17:06:29</td>\n",
       "      <td>discover delicious way help adult cat achieve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AE222MW56PH6JXPIB6XSAMCBTLNQ</td>\n",
       "      <td>B00GIHFUNG</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-12-07 02:15:25</td>\n",
       "      <td>inch wide standard black leather collar made l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  label  rating  \\\n",
       "0  AE22236AFRRSMQIKGG7TPTB75QEA  B0002C7FHC      1       5   \n",
       "1  AE22236AFRRSMQIKGG7TPTB75QEA  B00UFKQKLS      1       5   \n",
       "2  AE22236AFRRSMQIKGG7TPTB75QEA  B01I6X61OQ      1       5   \n",
       "5  AE22236AFRRSMQIKGG7TPTB75QEA  B0085JN2JY      0       0   \n",
       "6  AE222MW56PH6JXPIB6XSAMCBTLNQ  B00GIHFUNG      1       3   \n",
       "\n",
       "            timestamp                                               text  \n",
       "0 2009-09-19 19:42:10  dog whole lot say basic bark control collar ef...  \n",
       "1 2014-03-07 15:31:31  hamilton nylon comfort dog harness come vibran...  \n",
       "2 2014-03-07 17:06:29  high quality adjustable collar quickrelease bu...  \n",
       "5 2014-03-07 17:06:29  discover delicious way help adult cat achieve ...  \n",
       "6 2012-12-07 02:15:25  inch wide standard black leather collar made l...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_enriched_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d6a1b-c160-4d43-8a41-86401dad9299",
   "metadata": {},
   "source": [
    "## Creating Sequences & Vocabulary for Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98308dcc-c4e2-4d8a-bda9-ec1bd9f16001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb971cea-4211-41c8-8e4e-63e0ea26af5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload enriched cleaned data\n",
    "train_data = train_df_enriched_cleaned.copy()\n",
    "test_data = test_df_enriched_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7244f3-18ce-4ad8-a8e6-1e832528e04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding user_id and parent_asin\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_encoder.fit(pd.concat([train_data['user_id'], test_data['user_id']]))\n",
    "item_encoder.fit(pd.concat([train_data['parent_asin'], test_data['parent_asin']]))\n",
    "\n",
    "train_data['user_id_idx'] = user_encoder.transform(train_data['user_id'])\n",
    "train_data['item_id_idx'] = item_encoder.transform(train_data['parent_asin'])\n",
    "\n",
    "test_data['user_id_idx'] = user_encoder.transform(test_data['user_id'])\n",
    "test_data['item_id_idx'] = item_encoder.transform(test_data['parent_asin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6877f99-7b1d-4234-9757-be0eb721de18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vacab：111152\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# build vocabulary\n",
    "def build_vocab(sentences, max_vocab_size=1000000):\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        word_count.update(sent.split())\n",
    "    most_common_words = word_count.most_common(max_vocab_size - 2)  # 留出位置给 <PAD> 和 <UNK>\n",
    "    idx_to_word = ['<PAD>', '<UNK>'] + [word for word, _ in most_common_words]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "all_sentences = pd.concat([train_data['text'], test_data['text']])\n",
    "word_to_idx, idx_to_word = build_vocab(all_sentences)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"size of vacab：{vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193110ed-bfcc-4ff4-9060-7676dc83b6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 256  # set the max sequence length\n",
    "\n",
    "def text_to_sequence(text, word_to_idx, max_len):\n",
    "    tokens = text.split()\n",
    "    sequence = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in tokens]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [word_to_idx['<PAD>']] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    return sequence\n",
    "\n",
    "train_data['text_seq'] = train_data['text'].apply(lambda x: text_to_sequence(x, word_to_idx, MAX_SEQUENCE_LENGTH))\n",
    "test_data['text_seq'] = test_data['text'].apply(lambda x: text_to_sequence(x, word_to_idx, MAX_SEQUENCE_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47bffa9c-9546-4a7a-81c7-aa3c7ca0525e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_data, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47336d14-32b2-4312-a381-fc96cad0bb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    60553\n",
       "0    33322\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29808b-0660-4596-8dd4-ad2c5935ef48",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286ed0d9-26f6-453c-861c-6e257bfee28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create user-item pairs for training\n",
    "train_pairs = set(zip(train_data['user_id_idx'], train_data['item_id_idx']))\n",
    "\n",
    "# create user-item pairs for testing\n",
    "test_pairs = set(zip(test_data['user_id_idx'], test_data['item_id_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7465a-fa8a-47dd-93a0-927b2addc4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find the overlapped user-item pairs\n",
    "overlap_pairs = train_pairs.intersection(test_pairs)\n",
    "print(f\"number of overlapped pairs in training and testing：{len(overlap_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f90109e1-f608-4c18-92b9-60a6d7aef965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test_data after dedup：2646215\n"
     ]
    }
   ],
   "source": [
    "# drop overlap from testing set\n",
    "if len(overlap_pairs) > 0:\n",
    "    test_data_no_overlap = test_data[~test_data.apply(lambda row: (row['user_id_idx'], row['item_id_idx']) in overlap_pairs, axis=1)]\n",
    "    print(f\"length of test_data after dedup：{len(test_data_no_overlap)}\")\n",
    "else:\n",
    "    test_data_no_overlap = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d07b8ceb-1832-499d-9cc0-174a54cbfe96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_data after dedup：928436\n"
     ]
    }
   ],
   "source": [
    "# drop overlap from training set before split validation set\n",
    "train_data_no_overlap = train_data[~train_data.apply(lambda row: (row['user_id_idx'], row['item_id_idx']) in overlap_pairs, axis=1)]\n",
    "print(f\"length of train_data after dedup：{len(train_data_no_overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bf6e4d5-6c53-4eb1-baf6-a2f3f6092be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_data_no_overlap, test_size=0.1, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7cdfd8a-4fbb-4a3d-a4f2-637eb231a9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of overlapped pairs in training and validation：1240\n"
     ]
    }
   ],
   "source": [
    "train_pairs = set(zip(train_dataset['user_id_idx'], train_dataset['item_id_idx']))\n",
    "val_pairs = set(zip(val_dataset['user_id_idx'], val_dataset['item_id_idx']))\n",
    "overlap_train_val = train_pairs.intersection(val_pairs)\n",
    "print(f\"number of overlapped pairs in training and validation：{len(overlap_train_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a842865e-88a8-4794-918f-f51661f96a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_data after dedup：91581\n"
     ]
    }
   ],
   "source": [
    "if len(overlap_train_val) > 0:\n",
    "    val_dataset = val_dataset[~val_dataset.apply(lambda row: (row['user_id_idx'], row['item_id_idx']) in train_pairs, axis=1)]\n",
    "    print(f\"length of train_data after dedup：{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60cae000-f5ac-49d2-b4dc-7c8647b3845f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NCFDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, text_seqs, labels):\n",
    "        self.user_ids = user_ids\n",
    "        self.item_ids = item_ids\n",
    "        self.text_seqs = text_seqs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user_id': self.user_ids[idx],\n",
    "            'item_id': self.item_ids[idx],\n",
    "            'text_seq': torch.tensor(self.text_seqs[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea3d2a50-872f-430f-a4af-25ab97c47949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "279abfe9-69ce-4bcc-ad96-3f9cd3457cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset_obj = NCFDataset(\n",
    "    user_ids=train_dataset['user_id_idx'].values,\n",
    "    item_ids=train_dataset['item_id_idx'].values,\n",
    "    text_seqs=train_dataset['text_seq'].values,\n",
    "    labels=train_dataset['label'].values\n",
    ")\n",
    "train_loader = DataLoader(train_dataset_obj, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# validation\n",
    "val_dataset_obj = NCFDataset(\n",
    "    user_ids=val_dataset['user_id_idx'].values,\n",
    "    item_ids=val_dataset['item_id_idx'].values,\n",
    "    text_seqs=val_dataset['text_seq'].values,\n",
    "    labels=val_dataset['label'].values\n",
    ")\n",
    "val_loader = DataLoader(val_dataset_obj, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# test\n",
    "test_dataset_obj = NCFDataset(\n",
    "    user_ids=test_data_no_overlap['user_id_idx'].values,\n",
    "    item_ids=test_data_no_overlap['item_id_idx'].values,\n",
    "    text_seqs=test_data_no_overlap['text_seq'].values,\n",
    "    labels=test_data_no_overlap['label'].values,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset_obj, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e01ff6-64d5-4c9f-9538-d3da14d655c8",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20032c9a-4c52-43fb-bdd4-bf26510666e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, vocab_size, embedding_dim=32, text_embedding_dim=32):\n",
    "        super(NCFModel, self).__init__()\n",
    "        # user and item embedding\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # text embedding\n",
    "        self.text_embedding = nn.Embedding(vocab_size, text_embedding_dim, padding_idx=word_to_idx['<PAD>'])\n",
    "        self.conv1d = nn.Conv1d(in_channels=text_embedding_dim, out_channels=64, kernel_size=3)\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2 + 64, 128)  # 注意这里的输入维度\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, text_seqs):\n",
    "        # user and item embedding\n",
    "        user_embed = self.user_embedding(user_ids)\n",
    "        item_embed = self.item_embedding(item_ids)\n",
    "\n",
    "        # text embedding and convolution\n",
    "        text_embed = self.text_embedding(text_seqs).permute(0, 2, 1)\n",
    "        text_conv = self.conv1d(text_embed)\n",
    "        text_pool = self.pooling(text_conv).squeeze(2)\n",
    "\n",
    "        # concatenation\n",
    "        concat = torch.cat([user_embed, item_embed, text_pool], dim=1)\n",
    "\n",
    "        # fully connected layer\n",
    "        x = self.fc1(concat)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e18a5b76-793e-4b25-844e-826ed21870a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "model = NCFModel(num_users, num_items, vocab_size).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9225eff-81d3-4aaa-96f7-13fd88730365",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f156e26c-c9aa-4ee9-b651-e7b524206166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 3265/3265 [01:07<00:00, 48.26it/s, Loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7975, Recall: 0.8448, F1: 0.8205, AUC: 0.8371\n",
      "Found a better model, saved the current model. Best Recall: 0.8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.36it/s, Loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.8268, Recall: 0.8084, F1: 0.8175, AUC: 0.8391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.53it/s, Loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.8152, Recall: 0.8268, F1: 0.8210, AUC: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 3265/3265 [01:18<00:00, 41.73it/s, Loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7870, Recall: 0.8664, F1: 0.8248, AUC: 0.8364\n",
      "Found a better model, saved the current model. Best Recall: 0.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.74it/s, Loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7767, Recall: 0.8785, F1: 0.8245, AUC: 0.8321\n",
      "Found a better model, saved the current model. Best Recall: 0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.53it/s, Loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7701, Recall: 0.8861, F1: 0.8240, AUC: 0.8283\n",
      "Found a better model, saved the current model. Best Recall: 0.8861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.58it/s, Loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7662, Recall: 0.8924, F1: 0.8245, AUC: 0.8231\n",
      "Found a better model, saved the current model. Best Recall: 0.8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 3265/3265 [01:17<00:00, 42.09it/s, Loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7712, Recall: 0.8878, F1: 0.8254, AUC: 0.8172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.60it/s, Loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7739, Recall: 0.8492, F1: 0.8098, AUC: 0.8090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.66it/s, Loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7956, Recall: 0.7781, F1: 0.7868, AUC: 0.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.61it/s, Loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7905, Recall: 0.8010, F1: 0.7957, AUC: 0.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 3265/3265 [01:17<00:00, 41.89it/s, Loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7898, Recall: 0.7691, F1: 0.7793, AUC: 0.7916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.86it/s, Loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7924, Recall: 0.7638, F1: 0.7778, AUC: 0.7896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.42it/s, Loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7968, Recall: 0.7344, F1: 0.7643, AUC: 0.7836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 3265/3265 [01:10<00:00, 46.59it/s, Loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7899, Recall: 0.7427, F1: 0.7656, AUC: 0.7739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 3265/3265 [01:18<00:00, 41.74it/s, Loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7840, Recall: 0.7526, F1: 0.7680, AUC: 0.7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.84it/s, Loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7817, Recall: 0.7464, F1: 0.7637, AUC: 0.7645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.75it/s, Loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7808, Recall: 0.7331, F1: 0.7562, AUC: 0.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 3265/3265 [01:09<00:00, 46.70it/s, Loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7866, Recall: 0.7284, F1: 0.7564, AUC: 0.7542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 3265/3265 [01:18<00:00, 41.85it/s, Loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.7863, Recall: 0.7334, F1: 0.7589, AUC: 0.7525\n",
      "Training completed!\n",
      "Best model Recall: 0.8924\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "best_recall = 0.0  # Initialize best recall as 0\n",
    "best_model_path = './Results/NCF/best_model.pth'  # Define the model saving path\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        item_ids = batch['item_id'].to(device)\n",
    "        text_seqs = batch['text_seq'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_ids, item_ids, text_seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'Loss': total_loss / (pbar.n + 1)})\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            user_ids = batch['user_id'].to(device)\n",
    "            item_ids = batch['item_id'].to(device)\n",
    "            text_seqs = batch['text_seq'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(user_ids, item_ids, text_seqs)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    val_preds_binary = [1 if x > 0.5 else 0 for x in val_preds]\n",
    "    precision = precision_score(val_labels, val_preds_binary)\n",
    "    recall = recall_score(val_labels, val_preds_binary)\n",
    "    f1 = f1_score(val_labels, val_preds_binary)\n",
    "    auc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Save the best model based on recall\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Found a better model, saved the current model. Best Recall: {best_recall:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model Recall: {best_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb418d82-c7c2-4746-a962-38d42b9a2e4b",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47409db2-1f94-4440-9c1c-0ac8d4c13eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 10337/10337 [01:57<00:00, 87.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.7588, Recall: 0.8715, F1: 0.8113, AUC: 0.8146\n"
     ]
    }
   ],
   "source": [
    "# load the best NCF model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "# predict and validate on testing data\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        item_ids = batch['item_id'].to(device)\n",
    "        text_seqs = batch['text_seq'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(user_ids, item_ids, text_seqs)\n",
    "\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "# calculate eval metrics\n",
    "test_preds_binary = [1 if x > 0.5 else 0 for x in test_preds]\n",
    "precision = precision_score(test_labels, test_preds_binary)\n",
    "recall = recall_score(test_labels, test_preds_binary)\n",
    "f1 = f1_score(test_labels, test_preds_binary)\n",
    "auc = roc_auc_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c85a26-382c-46d5-bb1c-39745c57c86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 10337/10337 [02:06<00:00, 81.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5: 0.8119\n",
      "Average Recall@5: 0.7621\n",
      "Average Precision@10: 0.5498\n",
      "Average Recall@10: 0.9616\n",
      "Average Precision@15: 0.3885\n",
      "Average Recall@15: 0.9979\n"
     ]
    }
   ],
   "source": [
    "# load the best NCF model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "# define top K threshold\n",
    "K_values = [5, 10, 15]\n",
    "\n",
    "# restore testing results\n",
    "test_user_ids = []\n",
    "test_item_ids = []\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "# collect predictions on testing data\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        item_ids = batch['item_id'].to(device)\n",
    "        text_seqs = batch['text_seq'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(user_ids, item_ids, text_seqs)\n",
    "\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "        test_user_ids.extend(user_ids.cpu().numpy())\n",
    "        test_item_ids.extend(item_ids.cpu().numpy())\n",
    "\n",
    "# save testing results to df\n",
    "test_results_df = pd.DataFrame({\n",
    "    'user_id': test_user_ids,\n",
    "    'item_id': test_item_ids,\n",
    "    'label': test_labels,\n",
    "    'pred_score': test_preds\n",
    "})\n",
    "\n",
    "# test by user groups\n",
    "grouped = test_results_df.groupby('user_id')\n",
    "\n",
    "# save Precision@K and Recall@K for each k value to dict\n",
    "precision_at_K = {k: [] for k in K_values}\n",
    "recall_at_K = {k: [] for k in K_values}\n",
    "\n",
    "# for each user in each group\n",
    "for user_id, group in grouped:\n",
    "    user_df = group.sort_values('pred_score', ascending=False)\n",
    "    total_relevant = user_df['label'].sum()\n",
    "    # skip users without relevant items to avoid divided by 0\n",
    "    if total_relevant == 0:\n",
    "        continue\n",
    "    for K in K_values:\n",
    "        # get top K items\n",
    "        top_K = user_df.head(K)\n",
    "        # calculate number of relevant items in top K items\n",
    "        num_relevant_in_top_K = top_K['label'].sum()\n",
    "        # calculate Precision@K and Recall@K for each user\n",
    "        precision = num_relevant_in_top_K / K\n",
    "        recall = num_relevant_in_top_K / total_relevant\n",
    "        # save results\n",
    "        precision_at_K[K].append(precision)\n",
    "        recall_at_K[K].append(recall)\n",
    "\n",
    "# calculate average Precision@K and Recall@K for all users\n",
    "for K in K_values:\n",
    "    avg_precision = sum(precision_at_K[K]) / len(precision_at_K[K])\n",
    "    avg_recall = sum(recall_at_K[K]) / len(recall_at_K[K])\n",
    "    print(f\"Average Precision@{K}: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall@{K}: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b335778-f14f-4ba6-ba22-80313bb38d3f",
   "metadata": {},
   "source": [
    "# Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "838f7b1c-8c55-43d3-b0aa-fa18314070a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User AE22236AFRRSMQIKGG7TPTB75QEA has interacted with 18 items.\n",
      "User AE22236AFRRSMQIKGG7TPTB75QEA has 48311 unseen items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 48/48 [00:00<00:00, 171.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 recommendations for user AE22236AFRRSMQIKGG7TPTB75QEA:\n",
      "Item ID: B081PM9QD5, Predicted Score: 1.0000\n",
      "Item ID: B0C5K9Z2W4, Predicted Score: 1.0000\n",
      "Item ID: B07615C5Y9, Predicted Score: 1.0000\n",
      "Item ID: B013WNS1PW, Predicted Score: 1.0000\n",
      "Item ID: B0C8NNWS6W, Predicted Score: 0.9999\n",
      "Item ID: B00BUFTSV6, Predicted Score: 0.9999\n",
      "Item ID: B003FLM018, Predicted Score: 0.9999\n",
      "Item ID: B07CL37VB8, Predicted Score: 0.9999\n",
      "Item ID: B0053HLAA4, Predicted Score: 0.9999\n",
      "Item ID: B07NKXRG8J, Predicted Score: 0.9998\n",
      "Item ID: B0BFK266J3, Predicted Score: 0.9998\n",
      "Item ID: B0BJ16KKML, Predicted Score: 0.9998\n",
      "Item ID: B00YYIHGXS, Predicted Score: 0.9998\n",
      "Item ID: B07BMB1BZD, Predicted Score: 0.9998\n",
      "Item ID: B0BCPLFZD1, Predicted Score: 0.9998\n",
      "Item ID: B0BFYZY2DZ, Predicted Score: 0.9997\n",
      "Item ID: B0BNK5Y3R8, Predicted Score: 0.9996\n",
      "Item ID: B09D451T23, Predicted Score: 0.9996\n",
      "Item ID: B09HSRY37F, Predicted Score: 0.9995\n",
      "Item ID: B0016HNU12, Predicted Score: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1850/1725519691.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_N_items['parent_asin'] = item_encoder.inverse_transform(top_N_items['item_id_idx'])\n"
     ]
    }
   ],
   "source": [
    "# Select a specific user for generating recommendations\n",
    "specific_user_id = 'AE22236AFRRSMQIKGG7TPTB75QEA'  # Replace with the user ID for which you want recommendations\n",
    "user_idx = user_encoder.transform([specific_user_id])[0]  # Encode the user ID to its corresponding index\n",
    "\n",
    "# Get indices of all items\n",
    "all_item_indices = np.arange(len(item_encoder.classes_))\n",
    "\n",
    "# Retrieve items the user has interacted with in the training and test datasets\n",
    "user_train_items = train_data_no_overlap[train_data_no_overlap['user_id_idx'] == user_idx]['item_id_idx'].tolist()\n",
    "user_test_items = test_data_no_overlap[test_data_no_overlap['user_id_idx'] == user_idx]['item_id_idx'].tolist()\n",
    "\n",
    "user_interacted_items = set(user_train_items + user_test_items)  # Combine train and test interactions\n",
    "\n",
    "# Identify items the user has not interacted with\n",
    "user_unseen_items = np.setdiff1d(all_item_indices, list(user_interacted_items))  # Items not seen by the user\n",
    "\n",
    "print(f\"User {specific_user_id} has interacted with {len(user_interacted_items)} items.\")\n",
    "print(f\"User {specific_user_id} has {len(user_unseen_items)} unseen items.\")\n",
    "\n",
    "# Prepare input data for the model\n",
    "# Repeat the user index for all unseen items\n",
    "user_indices = np.full(len(user_unseen_items), user_idx, dtype=np.int64)\n",
    "item_indices = user_unseen_items\n",
    "\n",
    "# Prepare text sequences for unseen items\n",
    "# Create a DataFrame for unseen items\n",
    "unseen_items_df = pd.DataFrame({'item_id_idx': item_indices})\n",
    "\n",
    "# Retrieve item text information\n",
    "item_texts = filtered_data[['parent_asin', 'text']].drop_duplicates()  # Keep unique item text data\n",
    "item_texts['item_id_idx'] = item_encoder.transform(item_texts['parent_asin'])  # Encode item IDs\n",
    "\n",
    "# Merge text data with unseen items\n",
    "unseen_items_df = unseen_items_df.merge(item_texts[['item_id_idx', 'text']], on='item_id_idx', how='left')\n",
    "\n",
    "# Drop items without text\n",
    "unseen_items_df = unseen_items_df.dropna(subset=['text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "unseen_items_df['text_seq'] = unseen_items_df['text'].apply(lambda x: text_to_sequence(x, word_to_idx, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "# Update item and user indices to match the filtered unseen items\n",
    "item_indices = unseen_items_df['item_id_idx'].values\n",
    "user_indices = np.full(len(item_indices), user_idx, dtype=np.int64)\n",
    "text_seqs = np.stack(unseen_items_df['text_seq'].values)  # Stack sequences into a NumPy array\n",
    "\n",
    "# Convert to tensors for model input\n",
    "user_tensor = torch.tensor(user_indices, dtype=torch.long).to(device)\n",
    "item_tensor = torch.tensor(item_indices, dtype=torch.long).to(device)\n",
    "text_tensor = torch.tensor(text_seqs, dtype=torch.long).to(device)\n",
    "\n",
    "# Generate recommendations in batches if the data is too large\n",
    "batch_size = 1024\n",
    "num_batches = (len(user_indices) + batch_size - 1) // batch_size  # Calculate the number of batches\n",
    "\n",
    "all_scores = []  # Store scores for all unseen items\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_batches), desc='Generating recommendations'):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(user_indices))\n",
    "        batch_user = user_tensor[start_idx:end_idx]\n",
    "        batch_item = item_tensor[start_idx:end_idx]\n",
    "        batch_text = text_tensor[start_idx:end_idx]\n",
    "\n",
    "        # Get scores from the model\n",
    "        scores = model(batch_user, batch_item, batch_text)\n",
    "        all_scores.extend(scores.cpu().numpy())  # Append scores to the list\n",
    "\n",
    "# Add scores to the unseen items DataFrame\n",
    "unseen_items_df['score'] = all_scores\n",
    "\n",
    "# Sort items by score in descending order\n",
    "unseen_items_df = unseen_items_df.sort_values('score', ascending=False)\n",
    "\n",
    "# Get the top N recommendations\n",
    "N = 20\n",
    "top_N_items = unseen_items_df.head(N)\n",
    "\n",
    "# Map item indices back to original item IDs\n",
    "top_N_items['parent_asin'] = item_encoder.inverse_transform(top_N_items['item_id_idx'])\n",
    "\n",
    "# Display the top N recommendations\n",
    "print(f\"Top {N} recommendations for user {specific_user_id}:\")\n",
    "for idx, row in top_N_items.iterrows():\n",
    "    print(f\"Item ID: {row['parent_asin']}, Predicted Score: {row['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b01299-6c0d-4500-89f7-767c50fcc88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
