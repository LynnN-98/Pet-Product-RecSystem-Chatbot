{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a4cfa2-2951-4126-8502-834d33c263d5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0079d0b-498f-4c63-bb7d-55beed24fb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>questionType</th>\n",
       "      <th>asin</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>question</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_lang</th>\n",
       "      <th>answer_lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Oct 17, 2013</td>\n",
       "      <td>1.381993e+09</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Nov 10, 2013</td>\n",
       "      <td>1.384070e+09</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Jan 11, 2014</td>\n",
       "      <td>1.389427e+09</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: does this have any type of sugar, grane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Aug 7, 2013</td>\n",
       "      <td>1.375859e+09</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Does this contain citric acid?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Aug 20, 2014</td>\n",
       "      <td>1.408518e+09</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: IS this product VEGAN, specifically the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 questionType        asin    answerTime      unixTime  \\\n",
       "0          10   open-ended  4847676011  Oct 17, 2013  1.381993e+09   \n",
       "1          11   open-ended  4847676011  Nov 10, 2013  1.384070e+09   \n",
       "2          12   open-ended  4847676011  Jan 11, 2014  1.389427e+09   \n",
       "3          13       yes/no  4847676011   Aug 7, 2013  1.375859e+09   \n",
       "4          14       yes/no  4847676011  Aug 20, 2014  1.408518e+09   \n",
       "\n",
       "                                            question answerType  \\\n",
       "0                                Where is this made?        NaN   \n",
       "1  Does this have an expiration date? Does it onl...        NaN   \n",
       "2  does this have any type of sugar, grane alchol...        NaN   \n",
       "3                     Does this contain citric acid?          ?   \n",
       "4  IS this product VEGAN, specifically the glycerin?          ?   \n",
       "\n",
       "                                              answer question_lang  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...            en   \n",
       "1  yes it does have an expiration date. However, ...            en   \n",
       "2  Here is the list of ingredients: Sorbitol, hyd...            en   \n",
       "3                 it is not listed as an ingredient.            en   \n",
       "4  It is my understanding that this product is no...            en   \n",
       "\n",
       "  answer_lang                                               text  \n",
       "0          en  User: Where is this made?\\nAssistant: Made in ...  \n",
       "1          en  User: Does this have an expiration date? Does ...  \n",
       "2          en  User: does this have any type of sugar, grane ...  \n",
       "3          en  User: Does this contain citric acid?\\nAssistan...  \n",
       "4          en  User: IS this product VEGAN, specifically the ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_dir = '/home/sagemaker-user/Data/'\n",
    "model_dir = '/home/sagemaker-user/Models/'\n",
    "log_dir = '/home/sagemaker-user/Logs/'\n",
    "\n",
    "data = pd.read_csv(data_dir+'filtered_df.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed18a8c8-26b8-4edd-81e0-050f9afa44a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=data[['asin','question','answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60069fde-91c1-423e-8997-e15d570f286a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011                                Where is this made?   \n",
       "1  4847676011  Does this have an expiration date? Does it onl...   \n",
       "2  4847676011  does this have any type of sugar, grane alchol...   \n",
       "3  4847676011                     Does this contain citric acid?   \n",
       "4  4847676011  IS this product VEGAN, specifically the glycerin?   \n",
       "\n",
       "                                              answer  \n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...  \n",
       "1  yes it does have an expiration date. However, ...  \n",
       "2  Here is the list of ingredients: Sorbitol, hyd...  \n",
       "3                 it is not listed as an ingredient.  \n",
       "4  It is my understanding that this product is no...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a86623d-4a88-4318-85a1-8c10bc173e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dialogue text\n",
    "def create_dialogue_text(row):\n",
    "    return f\"User: {row['question']}\\nAssistant: {row['answer']}\\n\"\n",
    "\n",
    "data['dialogue'] = data.apply(create_dialogue_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e487778-1c25-4280-bacb-77352103c776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize training and test sets\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "# Split for each asin\n",
    "for asin, group in data.groupby('asin'):\n",
    "    # If sample size > 1 for this asin, split 80-20 for train/test\n",
    "    if len(group) > 1:\n",
    "        train_group, test_group = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        # If only one sample, add it to training set\n",
    "        train_group = group\n",
    "        test_group = pd.DataFrame(columns=group.columns)  # Empty test set portion\n",
    "    \n",
    "    # Add results to training and test set lists\n",
    "    train_data_list.append(train_group)\n",
    "    test_data_list.append(test_group)\n",
    "\n",
    "# Combine all asin training and test sets\n",
    "train_data = pd.concat(train_data_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_data_list).reset_index(drop=True)\n",
    "\n",
    "# View count comparison of each asin in training and test sets\n",
    "train_asin_counts = train_data['asin'].value_counts().reset_index()\n",
    "train_asin_counts.columns = ['asin', 'train_count']\n",
    "\n",
    "test_asin_counts = test_data['asin'].value_counts().reset_index()\n",
    "test_asin_counts.columns = ['asin', 'test_count']\n",
    "\n",
    "# Merge training and test set statistics\n",
    "asin_counts = pd.merge(train_asin_counts, test_asin_counts, on='asin', how='outer').fillna(0)\n",
    "asin_counts['train_count'] = asin_counts['train_count'].astype(int)\n",
    "asin_counts['test_count'] = asin_counts['test_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f932067-0939-4f50-9a6c-2640da7cec1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00006H373</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006JHRE</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  train_count  test_count\n",
       "0  4847676011            5           2\n",
       "1  B00004X14K            7           2\n",
       "2  B00006H36X            5           2\n",
       "3  B00006H373            4           1\n",
       "4  B00006JHRE            8           2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asin_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db01705a-8294-4315-9402-93693875e22e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does anyone know where this is made?</td>\n",
       "      <td>Believe it or not.... the USA!</td>\n",
       "      <td>User: does anyone know where this is made?\\nAs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "      <td>User: does this have any type of sugar, grane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "      <td>User: IS this product VEGAN, specifically the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "      <td>User: Does this contain citric acid?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>is this a paste or a gel?</td>\n",
       "      <td>It is a gel.</td>\n",
       "      <td>User: is this a paste or a gel?\\nAssistant: It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011               does anyone know where this is made?   \n",
       "1  4847676011  does this have any type of sugar, grane alchol...   \n",
       "2  4847676011  IS this product VEGAN, specifically the glycerin?   \n",
       "3  4847676011                     Does this contain citric acid?   \n",
       "4  4847676011                          is this a paste or a gel?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                     Believe it or not.... the USA!   \n",
       "1  Here is the list of ingredients: Sorbitol, hyd...   \n",
       "2  It is my understanding that this product is no...   \n",
       "3                 it is not listed as an ingredient.   \n",
       "4                                       It is a gel.   \n",
       "\n",
       "                                            dialogue  \n",
       "0  User: does anyone know where this is made?\\nAs...  \n",
       "1  User: does this have any type of sugar, grane ...  \n",
       "2  User: IS this product VEGAN, specifically the ...  \n",
       "3  User: Does this contain citric acid?\\nAssistan...  \n",
       "4  User: is this a paste or a gel?\\nAssistant: It...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e74dca-1bc0-44f4-88f1-8008f22bda92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>User: Are these containers BPA free?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>User: Is it airtight?\\nAssistant: Not air tigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>User: Want for something safe for my 18 year o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011                                Where is this made?   \n",
       "1  4847676011  Does this have an expiration date? Does it onl...   \n",
       "2  B00004X14K                     Are these containers BPA free?   \n",
       "3  B00004X14K                                    Is it airtight?   \n",
       "4  B00006H36X  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  User: Where is this made?\\nAssistant: Made in ...  \n",
       "1  User: Does this have an expiration date? Does ...  \n",
       "2  User: Are these containers BPA free?\\nAssistan...  \n",
       "3  User: Is it airtight?\\nAssistant: Not air tigh...  \n",
       "4  User: Want for something safe for my 18 year o...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27c04b-7280-4d61-8ca5-cfe6e1152077",
   "metadata": {},
   "source": [
    "## Tune Pre-trained GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a570a-81a2-4aa6-9281-46e83fc15bae",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a39aa7-6606-4605-aa76-2d7cde4c2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 21:27:39.253144: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-31 21:27:39.267911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-31 21:27:39.286396: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-31 21:27:39.292153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-31 21:27:39.305274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import contractions\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Loading complete')\n",
    "\n",
    "# Set random seed to ensure reproducible results\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf7963a-0c6f-42d5-aeb8-6a006b123b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Using cached anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Using cached pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Using cached pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd271027-307e-4324-baea-ec89704eb036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.17.0)\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181f80b-2445-4b0e-8475-b29b70aa4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4640e989-3830-4872-bd06-d1ba0500cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.3.1.post300)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e699e6ef-8fd7-4457-9942-acb41152f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.11/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2.3.1.post300)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Using cached accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Using cached accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.1.1 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 1.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.48.2 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 1.3.0 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.48.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8285c8b1-94e1-4767-9137-c4ba07cfe3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "261caf8e-cd20-4bca-9d13-34dd0b90320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (2.3.1.post300)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (0.26.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "106656cc-6fa7-49ba-ad55-4704764e4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.3.1.post300)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.48.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from bert_score) (3.9.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.26.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbd8c1-6569-4173-b819-9a0bd246b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3ebe2-e2a8-4ee6-8ba6-ca1edce4e59f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73e6772-8593-4658-8979-8d297a330747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/sagemaker-user/Data/'\n",
    "model_dir = '/home/sagemaker-user/Models/'\n",
    "log_dir = '/home/sagemaker-user/Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4024430-3001-4ea2-8839-f91374aa768f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bc235-fd72-4082-9e3c-6cf83f30814e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12059' max='210600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 12059/210600 2:28:36 < 40:47:06, 1.35 it/s, Epoch 11.45/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.273600</td>\n",
       "      <td>4.024562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.219600</td>\n",
       "      <td>3.937354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.039100</td>\n",
       "      <td>3.760592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.873800</td>\n",
       "      <td>3.639539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.740900</td>\n",
       "      <td>3.522080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.660400</td>\n",
       "      <td>3.484627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.634100</td>\n",
       "      <td>3.460740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.600900</td>\n",
       "      <td>3.443944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.588000</td>\n",
       "      <td>3.431790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.568200</td>\n",
       "      <td>3.422660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.558600</td>\n",
       "      <td>3.415689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(\n",
    "            dataframe['dialogue'].tolist(),\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.input_ids = self.encodings['input_ids']\n",
    "        self.attention_mask = self.encodings['attention_mask']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.input_ids[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset objects for training and validation sets\n",
    "train_dataset = ConversationDataset(train_data, tokenizer)\n",
    "val_dataset = ConversationDataset(test_data, tokenizer)\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Resize model's vocabulary to match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # More expressive adaptation\n",
    "    lora_alpha=32,  # Strengthen LoRA scaling\n",
    "    target_modules=[\"attn.c_proj\", \"attn.q_proj\", \"attn.k_proj\", \"attn.v_proj\"],  # Apply LoRA to more layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_all\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=200,  # Longer training\n",
    "    per_device_train_batch_size=16,  # Larger batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # Better weight updates\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=16,\n",
    "    label_smoothing_factor=0.0,  # Helps prevent overfitting\n",
    "    remove_unused_columns=False,  # Efficient batching\n",
    "    # max_grad_norm=1.0,  # Gradient clipping\n",
    ")\n",
    "\n",
    "# Calculate total training steps and warm-up steps\n",
    "total_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n",
    "warmup_steps = int(0.2 * total_steps)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Use DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Enable early stopping\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)  # Reduce patience steps\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9e42ff-560d-4012-a79c-e56887af829a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bb9aa-b5ca-484d-99a5-3c497668c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(model_dir+\"models_cli/gpt2\")\n",
    "tokenizer.save_pretrained(model_dir+\"models_cli/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "586a2e9a-2637-48c5-a93c-871e17456fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load sentence transformer model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize other models and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir+'models_cli/gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir+'models_cli/gpt2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0 if device == 'cuda' else -1)\n",
    "\n",
    "samples = test_data.reset_index(drop=True)\n",
    "\n",
    "# Separate user and assistant dialogues\n",
    "def extract_user_assistant(dialogue):\n",
    "    user_pattern = r'User:(.*?)\\n'\n",
    "    assistant_pattern = r'Assistant:(.*?)\\n'\n",
    "    \n",
    "    user_match = re.search(user_pattern, dialogue, re.DOTALL)\n",
    "    assistant_match = re.search(assistant_pattern, dialogue, re.DOTALL)\n",
    "    \n",
    "    user = user_match.group(1).strip() if user_match else ''\n",
    "    assistant = assistant_match.group(1).strip() if assistant_match else ''\n",
    "    \n",
    "    return user, assistant\n",
    "\n",
    "samples[['User', 'Assistant']] = samples['dialogue'].apply(\n",
    "    lambda x: pd.Series(extract_user_assistant(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df013028-eda8-4f18-935a-96667633c956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                User  \\\n",
       "0                                Where is this made?   \n",
       "1  Does this have an expiration date? Does it onl...   \n",
       "2                     Are these containers BPA free?   \n",
       "3                                    Is it airtight?   \n",
       "4  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                           Assistant  \n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...  \n",
       "1  yes it does have an expiration date. However, ...  \n",
       "2                               Sorry I do not know!  \n",
       "3  Not air tight, but it clicks closed. There is ...  \n",
       "4  the advantage II is a good product. have used ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[['User', 'Assistant']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ffbd2d0-c912-44cc-a626-31bd79a6fd4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "count    5.000000\n",
      "mean     0.224270\n",
      "std      0.202562\n",
      "min      0.003081\n",
      "25%      0.125000\n",
      "50%      0.222222\n",
      "75%      0.222222\n",
      "max      0.548827\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Generate model replies\n",
    "def generate_answer(question, tokenizer, model, device, max_length=150):\n",
    "    prompt = f\"User: {question}\\nAssistant:\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.encode('\\n')[0],\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "    answer = answer.split('\\n')[0]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "samples['Generated_Assistant'] = samples['User'].apply(lambda x: generate_answer(x, tokenizer, model, device))\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "def compute_metrics(row):\n",
    "    actual = row['Assistant']\n",
    "    generated = row['Generated_Assistant']\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(\n",
    "        [nltk.word_tokenize(actual.lower())],\n",
    "        nltk.word_tokenize(generated.lower()),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = rouge.score(actual, generated)\n",
    "    \n",
    "    # Calculate semantic similarity (Cosine Similarity)\n",
    "    cosine_sim = util.pytorch_cos_sim(\n",
    "        sbert_model.encode(actual, convert_to_tensor=True),\n",
    "        sbert_model.encode(generated, convert_to_tensor=True)\n",
    "    ).item()\n",
    "    \n",
    "    # Calculate BERTScore (commented out)\n",
    "    # P, R, F1 = bert_score([generated], [actual], lang=\"en\", verbose=False)\n",
    "    # bert_f1_score = F1.mean().item()\n",
    "    \n",
    "    # Return single values instead of Series\n",
    "    return {\n",
    "        'BLEU': float(bleu_score),  # Ensure float return type\n",
    "        'ROUGE-1': float(rouge_scores['rouge1'].fmeasure),\n",
    "        'ROUGE-2': float(rouge_scores['rouge2'].fmeasure),\n",
    "        'ROUGE-L': float(rouge_scores['rougeL'].fmeasure),\n",
    "        'Cosine_Similarity': float(cosine_sim),\n",
    "        # 'BERTScore_F1': bert_f1_score\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each sample\n",
    "metrics_results = []\n",
    "for idx, row in samples.iterrows():\n",
    "    metrics = compute_metrics(row)\n",
    "    metrics_results.append(metrics)\n",
    "\n",
    "# Convert metrics to DataFrame and add to samples data\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "samples = samples.assign(**metrics_df)\n",
    "\n",
    "\n",
    "# # Output results for each sample and format metrics\n",
    "# for i in range(sample_size):\n",
    "#     row = samples.iloc[i]\n",
    "#     print(f\"\\nSample {i+1}:\")\n",
    "#     print(f\"User: {row['User']}\")\n",
    "#     print(f\"Actual Assistant: {row['Assistant']}\")\n",
    "#     print(f\"Generated Assistant: {row['Generated_Assistant']}\")\n",
    "#     print(\"\\nMetrics:\")\n",
    "    \n",
    "#     # Convert Series to float format for formatted output\n",
    "#     metrics = {\n",
    "#         'BLEU': float(row['BLEU'].iloc[0] if isinstance(row['BLEU'], pd.Series) else row['BLEU']),\n",
    "#         'ROUGE-1': float(row['ROUGE-1'].iloc[0] if isinstance(row['ROUGE-1'], pd.Series) else row['ROUGE-1']),\n",
    "#         'ROUGE-2': float(row['ROUGE-2'].iloc[0] if isinstance(row['ROUGE-2'], pd.Series) else row['ROUGE-2']),\n",
    "#         'ROUGE-L': float(row['ROUGE-L'].iloc[0] if isinstance(row['ROUGE-L'], pd.Series) else row['ROUGE-L']),\n",
    "#         'Cosine_Similarity': float(row['Cosine_Similarity'].iloc[0] if isinstance(row['Cosine_Similarity'], pd.Series) else row['Cosine_Similarity']),\n",
    "#         # 'BERTScore_F1': float(row['BERTScore_F1'].iloc[0] if isinstance(row['BERTScore_F1'], pd.Series) else row['BERTScore_F1'])\n",
    "#     }\n",
    "    \n",
    "#     # Output formatted metrics\n",
    "#     print(f\"BLEU Score: {metrics['BLEU']:.4f}\")\n",
    "#     print(f\"ROUGE-1: {metrics['ROUGE-1']:.4f}\")\n",
    "#     print(f\"ROUGE-2: {metrics['ROUGE-2']:.4f}\")\n",
    "#     print(f\"ROUGE-L: {metrics['ROUGE-L']:.4f}\")\n",
    "#     print(f\"Cosine Similarity: {metrics['Cosine_Similarity']:.4f}\")\n",
    "#     # print(f\"BERTScore (F1): {metrics['BERTScore_F1']:.4f}\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "metrics_columns = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine_Similarity']\n",
    "summary_stats = samples[metrics_columns].apply(lambda x: pd.to_numeric(x.iloc[0] if isinstance(x, pd.Series) else x)).describe()\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d02ff3f-b35b-4318-bec3-bb11464eb142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine_Similarity']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfaa3c4b-9476-4ef2-8dac-0596bb281c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.019936</td>\n",
       "      <td>0.196107</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.148743</td>\n",
       "      <td>0.367421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030105</td>\n",
       "      <td>0.103517</td>\n",
       "      <td>0.057322</td>\n",
       "      <td>0.084737</td>\n",
       "      <td>0.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.192230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.005946</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095482</td>\n",
       "      <td>0.225701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.191553</td>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.141240</td>\n",
       "      <td>0.373625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.024359</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.187634</td>\n",
       "      <td>0.508313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              BLEU      ROUGE-1      ROUGE-2      ROUGE-L  Cosine_Similarity\n",
       "count  5124.000000  5124.000000  5124.000000  5124.000000        5124.000000\n",
       "mean      0.019936     0.196107     0.036671     0.148743           0.367421\n",
       "std       0.030105     0.103517     0.057322     0.084737           0.198500\n",
       "min       0.000000     0.000000     0.000000     0.000000          -0.192230\n",
       "25%       0.005946     0.125000     0.000000     0.095482           0.225701\n",
       "50%       0.013544     0.191553     0.018780     0.141240           0.373625\n",
       "75%       0.024359     0.260870     0.054545     0.187634           0.508313\n",
       "max       1.000000     1.000000     1.000000     1.000000           1.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8eba3fa-2b75-4d71-b880-bb0988ee653b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "      <th>Generated_Assistant</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>Made in China.</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.548827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>I have not had this issue with my cat, so I ca...</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.122072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>User: Are these containers BPA free?\\nAssistan...</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>They are not. I have been using them for over ...</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.140181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>User: Is it airtight?\\nAssistant: Not air tigh...</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>It is not airtight. It is a very sturdy piece ...</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.594254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>User: Want for something safe for my 18 year o...</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>I would recommend that you look at the safety ...</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.282334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011                                Where is this made?   \n",
       "1  4847676011  Does this have an expiration date? Does it onl...   \n",
       "2  B00004X14K                     Are these containers BPA free?   \n",
       "3  B00004X14K                                    Is it airtight?   \n",
       "4  B00006H36X  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                            dialogue  \\\n",
       "0  User: Where is this made?\\nAssistant: Made in ...   \n",
       "1  User: Does this have an expiration date? Does ...   \n",
       "2  User: Are these containers BPA free?\\nAssistan...   \n",
       "3  User: Is it airtight?\\nAssistant: Not air tigh...   \n",
       "4  User: Want for something safe for my 18 year o...   \n",
       "\n",
       "                                                User  \\\n",
       "0                                Where is this made?   \n",
       "1  Does this have an expiration date? Does it onl...   \n",
       "2                     Are these containers BPA free?   \n",
       "3                                    Is it airtight?   \n",
       "4  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                           Assistant  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                 Generated_Assistant      BLEU   ROUGE-1  \\\n",
       "0                                     Made in China.  0.003081  0.222222   \n",
       "1  I have not had this issue with my cat, so I ca...  0.020189  0.204545   \n",
       "2  They are not. I have been using them for over ...  0.019731  0.222222   \n",
       "3  It is not airtight. It is a very sturdy piece ...  0.012754  0.250000   \n",
       "4  I would recommend that you look at the safety ...  0.006321  0.144737   \n",
       "\n",
       "    ROUGE-2   ROUGE-L  Cosine_Similarity  \n",
       "0  0.125000  0.222222           0.548827  \n",
       "1  0.000000  0.159091           0.122072  \n",
       "2  0.000000  0.111111           0.140181  \n",
       "3  0.076923  0.225000           0.594254  \n",
       "4  0.000000  0.105263           0.282334  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8224691d-ac4e-404c-bdfd-c1914b904c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_samples = samples.sort_values(by='BLEU', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17284593-b220-4cff-9d5b-51f5703456d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "      <th>Generated_Assistant</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>B001CHXJSK</td>\n",
       "      <td>Does this filter come with one or more bio-bag...</td>\n",
       "      <td>Yes, it comes with one.</td>\n",
       "      <td>User: Does this filter come with one or more b...</td>\n",
       "      <td>Does this filter come with one or more bio-bag...</td>\n",
       "      <td>Yes, it comes with one.</td>\n",
       "      <td>Yes, it comes with one.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>B000FPH2I8</td>\n",
       "      <td>is it made in the USA</td>\n",
       "      <td>Yes it is made in the USA and is a great product.</td>\n",
       "      <td>User: is it made in the USA\\nAssistant: Yes it...</td>\n",
       "      <td>is it made in the USA</td>\n",
       "      <td>Yes it is made in the USA and is a great product.</td>\n",
       "      <td>Yes it is made in the USA.</td>\n",
       "      <td>0.465379</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.942750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542</th>\n",
       "      <td>B00AN0PRNW</td>\n",
       "      <td>Is this a bark collar as well in that it preve...</td>\n",
       "      <td>No, not a bark collar.</td>\n",
       "      <td>User: Is this a bark collar as well in that it...</td>\n",
       "      <td>Is this a bark collar as well in that it preve...</td>\n",
       "      <td>No, not a bark collar.</td>\n",
       "      <td>Yes, it is a bark collar.</td>\n",
       "      <td>0.365555</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.943956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>B002RT8M9I</td>\n",
       "      <td>Can you tell the water level by looking at it?...</td>\n",
       "      <td>Yes, it is clear enough to tell</td>\n",
       "      <td>User: Can you tell the water level by looking ...</td>\n",
       "      <td>Can you tell the water level by looking at it?...</td>\n",
       "      <td>Yes, it is clear enough to tell</td>\n",
       "      <td>Yes, it is solid.</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.316334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>B000F8IXOW</td>\n",
       "      <td>Is it made in United States?</td>\n",
       "      <td>Yes my label says made in the USA</td>\n",
       "      <td>User: Is it made in United States?\\nAssistant:...</td>\n",
       "      <td>Is it made in United States?</td>\n",
       "      <td>Yes my label says made in the USA</td>\n",
       "      <td>It is made in the USA.</td>\n",
       "      <td>0.356403</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.739864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            asin                                           question  \\\n",
       "1835  B001CHXJSK  Does this filter come with one or more bio-bag...   \n",
       "984   B000FPH2I8                              is it made in the USA   \n",
       "4542  B00AN0PRNW  Is this a bark collar as well in that it preve...   \n",
       "2341  B002RT8M9I  Can you tell the water level by looking at it?...   \n",
       "957   B000F8IXOW                       Is it made in United States?   \n",
       "\n",
       "                                                 answer  \\\n",
       "1835                            Yes, it comes with one.   \n",
       "984   Yes it is made in the USA and is a great product.   \n",
       "4542                             No, not a bark collar.   \n",
       "2341                    Yes, it is clear enough to tell   \n",
       "957                   Yes my label says made in the USA   \n",
       "\n",
       "                                               dialogue  \\\n",
       "1835  User: Does this filter come with one or more b...   \n",
       "984   User: is it made in the USA\\nAssistant: Yes it...   \n",
       "4542  User: Is this a bark collar as well in that it...   \n",
       "2341  User: Can you tell the water level by looking ...   \n",
       "957   User: Is it made in United States?\\nAssistant:...   \n",
       "\n",
       "                                                   User  \\\n",
       "1835  Does this filter come with one or more bio-bag...   \n",
       "984                               is it made in the USA   \n",
       "4542  Is this a bark collar as well in that it preve...   \n",
       "2341  Can you tell the water level by looking at it?...   \n",
       "957                        Is it made in United States?   \n",
       "\n",
       "                                              Assistant  \\\n",
       "1835                            Yes, it comes with one.   \n",
       "984   Yes it is made in the USA and is a great product.   \n",
       "4542                             No, not a bark collar.   \n",
       "2341                    Yes, it is clear enough to tell   \n",
       "957                   Yes my label says made in the USA   \n",
       "\n",
       "             Generated_Assistant      BLEU   ROUGE-1   ROUGE-2   ROUGE-L  \\\n",
       "1835     Yes, it comes with one.  1.000000  1.000000  1.000000  1.000000   \n",
       "984   Yes it is made in the USA.  0.465379  0.736842  0.705882  0.736842   \n",
       "4542   Yes, it is a bark collar.  0.365555  0.545455  0.444444  0.545455   \n",
       "2341           Yes, it is solid.  0.364093  0.545455  0.444444  0.545455   \n",
       "957       It is made in the USA.  0.356403  0.571429  0.500000  0.571429   \n",
       "\n",
       "      Cosine_Similarity  \n",
       "1835           1.000000  \n",
       "984            0.942750  \n",
       "4542           0.943956  \n",
       "2341           0.316334  \n",
       "957            0.739864  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97266506-7d0e-47be-b7eb-9077fe1ac72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
