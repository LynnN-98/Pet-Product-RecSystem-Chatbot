{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a4cfa2-2951-4126-8502-834d33c263d5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0079d0b-498f-4c63-bb7d-55beed24fb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>questionType</th>\n",
       "      <th>asin</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>question</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_lang</th>\n",
       "      <th>answer_lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Oct 17, 2013</td>\n",
       "      <td>1.381993e+09</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Nov 10, 2013</td>\n",
       "      <td>1.384070e+09</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Jan 11, 2014</td>\n",
       "      <td>1.389427e+09</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: does this have any type of sugar, grane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Aug 7, 2013</td>\n",
       "      <td>1.375859e+09</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: Does this contain citric acid?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>4847676011</td>\n",
       "      <td>Aug 20, 2014</td>\n",
       "      <td>1.408518e+09</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>User: IS this product VEGAN, specifically the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 questionType        asin    answerTime      unixTime  \\\n",
       "0          10   open-ended  4847676011  Oct 17, 2013  1.381993e+09   \n",
       "1          11   open-ended  4847676011  Nov 10, 2013  1.384070e+09   \n",
       "2          12   open-ended  4847676011  Jan 11, 2014  1.389427e+09   \n",
       "3          13       yes/no  4847676011   Aug 7, 2013  1.375859e+09   \n",
       "4          14       yes/no  4847676011  Aug 20, 2014  1.408518e+09   \n",
       "\n",
       "                                            question answerType  \\\n",
       "0                                Where is this made?        NaN   \n",
       "1  Does this have an expiration date? Does it onl...        NaN   \n",
       "2  does this have any type of sugar, grane alchol...        NaN   \n",
       "3                     Does this contain citric acid?          ?   \n",
       "4  IS this product VEGAN, specifically the glycerin?          ?   \n",
       "\n",
       "                                              answer question_lang  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...            en   \n",
       "1  yes it does have an expiration date. However, ...            en   \n",
       "2  Here is the list of ingredients: Sorbitol, hyd...            en   \n",
       "3                 it is not listed as an ingredient.            en   \n",
       "4  It is my understanding that this product is no...            en   \n",
       "\n",
       "  answer_lang                                               text  \n",
       "0          en  User: Where is this made?\\nAssistant: Made in ...  \n",
       "1          en  User: Does this have an expiration date? Does ...  \n",
       "2          en  User: does this have any type of sugar, grane ...  \n",
       "3          en  User: Does this contain citric acid?\\nAssistan...  \n",
       "4          en  User: IS this product VEGAN, specifically the ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('filtered_df.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed18a8c8-26b8-4edd-81e0-050f9afa44a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=data[['asin','question','answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60069fde-91c1-423e-8997-e15d570f286a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21960</th>\n",
       "      <td>B00JZA00HE</td>\n",
       "      <td>Does this have only one level of correction or...</td>\n",
       "      <td>It has 6 levels... But I think they are for se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21961</th>\n",
       "      <td>B00JZA00HE</td>\n",
       "      <td>Swimming, will it hold up to my dog swimming w...</td>\n",
       "      <td>Fully immersed in water, absolutely not. They ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>B00JZA00HE</td>\n",
       "      <td>Is this product waterproof?</td>\n",
       "      <td>We left it out in the rain on our dog and it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21963</th>\n",
       "      <td>B00JZA00HE</td>\n",
       "      <td>Is this too bulky for a 5 lb. Chihuahua? Will ...</td>\n",
       "      <td>It is bulky for a small dog and it is a piece ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21964</th>\n",
       "      <td>B00JZA00HE</td>\n",
       "      <td>I do not understand the high-low sensitivity s...</td>\n",
       "      <td>In theory, if the dog barks once, he gets a to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21965 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin                                           question  \\\n",
       "0      4847676011                                Where is this made?   \n",
       "1      4847676011  Does this have an expiration date? Does it onl...   \n",
       "2      4847676011  does this have any type of sugar, grane alchol...   \n",
       "3      4847676011                     Does this contain citric acid?   \n",
       "4      4847676011  IS this product VEGAN, specifically the glycerin?   \n",
       "...           ...                                                ...   \n",
       "21960  B00JZA00HE  Does this have only one level of correction or...   \n",
       "21961  B00JZA00HE  Swimming, will it hold up to my dog swimming w...   \n",
       "21962  B00JZA00HE                        Is this product waterproof?   \n",
       "21963  B00JZA00HE  Is this too bulky for a 5 lb. Chihuahua? Will ...   \n",
       "21964  B00JZA00HE  I do not understand the high-low sensitivity s...   \n",
       "\n",
       "                                                  answer  \n",
       "0      Made in USA Distributed by: Sergeant's Pet Car...  \n",
       "1      yes it does have an expiration date. However, ...  \n",
       "2      Here is the list of ingredients: Sorbitol, hyd...  \n",
       "3                     it is not listed as an ingredient.  \n",
       "4      It is my understanding that this product is no...  \n",
       "...                                                  ...  \n",
       "21960  It has 6 levels... But I think they are for se...  \n",
       "21961  Fully immersed in water, absolutely not. They ...  \n",
       "21962  We left it out in the rain on our dog and it s...  \n",
       "21963  It is bulky for a small dog and it is a piece ...  \n",
       "21964  In theory, if the dog barks once, he gets a to...  \n",
       "\n",
       "[21965 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a86623d-4a88-4318-85a1-8c10bc173e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dialogue text\n",
    "def create_dialogue_text(row):\n",
    "    return f\"User: {row['question']}\\nAssistant: {row['answer']}\\n\"\n",
    "\n",
    "data['dialogue'] = data.apply(create_dialogue_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e487778-1c25-4280-bacb-77352103c776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize training and test sets\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "# Split for each asin\n",
    "for asin, group in data.groupby('asin'):\n",
    "    # If sample size > 1 for this asin, split 80-20 for train/test\n",
    "    if len(group) > 1:\n",
    "        train_group, test_group = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        # If only one sample, add it to training set\n",
    "        train_group = group\n",
    "        test_group = pd.DataFrame(columns=group.columns)  # Empty test set portion\n",
    "    \n",
    "    # Add results to training and test set lists\n",
    "    train_data_list.append(train_group)\n",
    "    test_data_list.append(test_group)\n",
    "\n",
    "# Combine all asin training and test sets\n",
    "train_data = pd.concat(train_data_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_data_list).reset_index(drop=True)\n",
    "\n",
    "# View count comparison of each asin in training and test sets\n",
    "train_asin_counts = train_data['asin'].value_counts().reset_index()\n",
    "train_asin_counts.columns = ['asin', 'train_count']\n",
    "\n",
    "test_asin_counts = test_data['asin'].value_counts().reset_index()\n",
    "test_asin_counts.columns = ['asin', 'test_count']\n",
    "\n",
    "# Merge training and test set statistics\n",
    "asin_counts = pd.merge(train_asin_counts, test_asin_counts, on='asin', how='outer').fillna(0)\n",
    "asin_counts['train_count'] = asin_counts['train_count'].astype(int)\n",
    "asin_counts['test_count'] = asin_counts['test_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f932067-0939-4f50-9a6c-2640da7cec1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00006H373</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006JHRE</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  train_count  test_count\n",
       "0  4847676011            5           2\n",
       "1  B00004X14K            7           2\n",
       "2  B00006H36X            5           2\n",
       "3  B00006H373            4           1\n",
       "4  B00006JHRE            8           2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asin_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db01705a-8294-4315-9402-93693875e22e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does anyone know where this is made?</td>\n",
       "      <td>Believe it or not.... the USA!</td>\n",
       "      <td>User: does anyone know where this is made?\\nAs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>does this have any type of sugar, grane alchol...</td>\n",
       "      <td>Here is the list of ingredients: Sorbitol, hyd...</td>\n",
       "      <td>User: does this have any type of sugar, grane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>IS this product VEGAN, specifically the glycerin?</td>\n",
       "      <td>It is my understanding that this product is no...</td>\n",
       "      <td>User: IS this product VEGAN, specifically the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this contain citric acid?</td>\n",
       "      <td>it is not listed as an ingredient.</td>\n",
       "      <td>User: Does this contain citric acid?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>is this a paste or a gel?</td>\n",
       "      <td>It is a gel.</td>\n",
       "      <td>User: is this a paste or a gel?\\nAssistant: It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011               does anyone know where this is made?   \n",
       "1  4847676011  does this have any type of sugar, grane alchol...   \n",
       "2  4847676011  IS this product VEGAN, specifically the glycerin?   \n",
       "3  4847676011                     Does this contain citric acid?   \n",
       "4  4847676011                          is this a paste or a gel?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                     Believe it or not.... the USA!   \n",
       "1  Here is the list of ingredients: Sorbitol, hyd...   \n",
       "2  It is my understanding that this product is no...   \n",
       "3                 it is not listed as an ingredient.   \n",
       "4                                       It is a gel.   \n",
       "\n",
       "                                            dialogue  \n",
       "0  User: does anyone know where this is made?\\nAs...  \n",
       "1  User: does this have any type of sugar, grane ...  \n",
       "2  User: IS this product VEGAN, specifically the ...  \n",
       "3  User: Does this contain citric acid?\\nAssistan...  \n",
       "4  User: is this a paste or a gel?\\nAssistant: It...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6e74dca-1bc0-44f4-88f1-8008f22bda92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>User: Are these containers BPA free?\\nAssistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>User: Is it airtight?\\nAssistant: Not air tigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>User: Want for something safe for my 18 year o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011                                Where is this made?   \n",
       "1  4847676011  Does this have an expiration date? Does it onl...   \n",
       "2  B00004X14K                     Are these containers BPA free?   \n",
       "3  B00004X14K                                    Is it airtight?   \n",
       "4  B00006H36X  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  User: Where is this made?\\nAssistant: Made in ...  \n",
       "1  User: Does this have an expiration date? Does ...  \n",
       "2  User: Are these containers BPA free?\\nAssistan...  \n",
       "3  User: Is it airtight?\\nAssistant: Not air tigh...  \n",
       "4  User: Want for something safe for my 18 year o...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27c04b-7280-4d61-8ca5-cfe6e1152077",
   "metadata": {},
   "source": [
    "## Tune Pre-trained GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a570a-81a2-4aa6-9281-46e83fc15bae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aef535b-c14c-4a1a-b0f6-3fa21f2e447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.3.1.post300)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b71e53-6e07-4822-bc41-77acf010bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.3.1.post300)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from bert_score) (3.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bert_score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.25.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055d1d68-33bb-4042-9a69-59859db1f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (4.66.5)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ceeefb798c08648a00585436658103badad58d99577f2ddf0a9a117b79b38861\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44147bbc-05a7-4b9d-9dd0-f1fe6a306509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (2.3.1.post300)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4083910f-1a92-4c8c-bf94-937694a27ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 22:54:40.770708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 22:54:40.784821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-13 22:54:40.802622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-13 22:54:40.808119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 22:54:40.820615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import contractions\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Loading complete')\n",
    "\n",
    "# Set random seed to ensure reproducible results\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fdbd8c1-6569-4173-b819-9a0bd246b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3ebe2-e2a8-4ee6-8ba6-ca1edce4e59f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e6772-8593-4658-8979-8d297a330747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/sagemaker-user/Data/'\n",
    "model_dir = '/home/sagemaker-user/Models/'\n",
    "log_dir = '/home/sagemaker-user/Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4024430-3001-4ea2-8839-f91374aa768f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bc235-fd72-4082-9e3c-6cf83f30814e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39753' max='105300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 39753/105300 8:11:04 < 13:29:45, 1.35 it/s, Epoch 37.75/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.252600</td>\n",
       "      <td>3.976957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.994200</td>\n",
       "      <td>3.693734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.733200</td>\n",
       "      <td>3.512443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.632100</td>\n",
       "      <td>3.465721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.595300</td>\n",
       "      <td>3.440220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.575300</td>\n",
       "      <td>3.423561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.564700</td>\n",
       "      <td>3.412681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.539400</td>\n",
       "      <td>3.404006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.532400</td>\n",
       "      <td>3.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.517700</td>\n",
       "      <td>3.389813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.510600</td>\n",
       "      <td>3.384685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.500200</td>\n",
       "      <td>3.380003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.492300</td>\n",
       "      <td>3.376374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.491600</td>\n",
       "      <td>3.372610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.486300</td>\n",
       "      <td>3.368546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.480700</td>\n",
       "      <td>3.365918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.471300</td>\n",
       "      <td>3.362920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.473400</td>\n",
       "      <td>3.359561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>3.358388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.473400</td>\n",
       "      <td>3.355651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.463400</td>\n",
       "      <td>3.353530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.465600</td>\n",
       "      <td>3.351269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.455000</td>\n",
       "      <td>3.349490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.459000</td>\n",
       "      <td>3.347091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.459200</td>\n",
       "      <td>3.346235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.447000</td>\n",
       "      <td>3.344030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.457600</td>\n",
       "      <td>3.342838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.449300</td>\n",
       "      <td>3.340978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>3.339352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.448000</td>\n",
       "      <td>3.338434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.445200</td>\n",
       "      <td>3.336818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.434800</td>\n",
       "      <td>3.335978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.436300</td>\n",
       "      <td>3.334604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.435000</td>\n",
       "      <td>3.333612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.429400</td>\n",
       "      <td>3.332086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.441000</td>\n",
       "      <td>3.332062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.431800</td>\n",
       "      <td>3.330447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            self.training_losses.append(logs['loss'])\n",
    "        if logs and 'eval_loss' in logs:\n",
    "            self.validation_losses.append(logs['eval_loss'])\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(\n",
    "            dataframe['dialogue'].tolist(),\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.input_ids = self.encodings['input_ids']\n",
    "        self.attention_mask = self.encodings['attention_mask']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.input_ids[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset objects for training and validation sets\n",
    "train_dataset = ConversationDataset(train_data, tokenizer)\n",
    "val_dataset = ConversationDataset(test_data, tokenizer)\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Resize model's vocabulary to match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Adjust to 8\n",
    "    lora_alpha=16,  # Adjust to 16\n",
    "    target_modules=[\"attn.c_proj\"],  # Ensure correct target modules\n",
    "    lora_dropout=0.1,  # Reduce dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir+\"gpt2_all\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=16,\n",
    "    label_smoothing_factor=0.0,\n",
    "    remove_unused_columns=False,  # Add this line\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate total training steps and warm-up steps\n",
    "total_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Use DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Enable early stopping\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)  # Reduce patience steps\n",
    "\n",
    "# Initialize the loss logger\n",
    "loss_logger = LossLoggerCallback()\n",
    "\n",
    "# Define Trainer with the loss logger callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[early_stopping, loss_logger],  # Add loss logger here\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Plot the losses after training\n",
    "epochs = range(1, len(loss_logger.training_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_logger.training_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, loss_logger.validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e42ff-560d-4012-a79c-e56887af829a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bb9aa-b5ca-484d-99a5-3c497668c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(model_dir+\"models_cli/gpt2\")\n",
    "tokenizer.save_pretrained(model_dir+\"models_cli/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586a2e9a-2637-48c5-a93c-871e17456fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659b562222ac499c8279aef2c9e4cbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd66b106a24ff68decbf40b5e0bdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821488ef1a0a4b7d99bf73d994a9f828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cb9d2cfa3d4a4f9444c64968d1dbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f7acef9f60474a9452078c2d17949a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/360 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52f73139bb84279b97f8a9859073262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4249305820942b89de98aa62ddc2b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3261f693a5d0401da18a2e7f5cccf0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb8e953b4444124af817868690f9bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7219d718c15344d8b88fd02db19f1a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447d74d24af4478298a0907187484b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load sentence transformer model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize other models and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir+\"models_cli/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir+\"models_cli/gpt2\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0 if device == 'cuda' else -1)\n",
    "\n",
    "samples = test_data.reset_index(drop=True)\n",
    "\n",
    "# Separate user and assistant dialogues\n",
    "def extract_user_assistant(dialogue):\n",
    "    user_pattern = r'User:(.*?)\\n'\n",
    "    assistant_pattern = r'Assistant:(.*?)\\n'\n",
    "    \n",
    "    user_match = re.search(user_pattern, dialogue, re.DOTALL)\n",
    "    assistant_match = re.search(assistant_pattern, dialogue, re.DOTALL)\n",
    "    \n",
    "    user = user_match.group(1).strip() if user_match else ''\n",
    "    assistant = assistant_match.group(1).strip() if assistant_match else ''\n",
    "    \n",
    "    return user, assistant\n",
    "\n",
    "samples[['User', 'Assistant']] = samples['dialogue'].apply(\n",
    "    lambda x: pd.Series(extract_user_assistant(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df013028-eda8-4f18-935a-96667633c956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                User  \\\n",
       "0                                Where is this made?   \n",
       "1  Does this have an expiration date? Does it onl...   \n",
       "2                     Are these containers BPA free?   \n",
       "3                                    Is it airtight?   \n",
       "4  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                           Assistant  \n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...  \n",
       "1  yes it does have an expiration date. However, ...  \n",
       "2                               Sorry I do not know!  \n",
       "3  Not air tight, but it clicks closed. There is ...  \n",
       "4  the advantage II is a good product. have used ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[['User', 'Assistant']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ffbd2d0-c912-44cc-a626-31bd79a6fd4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "count    5.000000\n",
      "mean     0.071472\n",
      "std      0.159817\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      0.000000\n",
      "max      0.357362\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Generate model replies\n",
    "def generate_answer(question, tokenizer, model, device, max_length=150):\n",
    "    prompt = f\"User: {question}\\nAssistant:\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.encode('\\n')[0],\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "    answer = answer.split('\\n')[0]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "samples['Generated_Assistant'] = samples['User'].apply(lambda x: generate_answer(x, tokenizer, model, device))\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "def compute_metrics(row):\n",
    "    actual = row['Assistant']\n",
    "    generated = row['Generated_Assistant']\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(\n",
    "        [nltk.word_tokenize(actual.lower())],\n",
    "        nltk.word_tokenize(generated.lower()),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = rouge.score(actual, generated)\n",
    "    \n",
    "    # Calculate semantic similarity (Cosine Similarity)\n",
    "    cosine_sim = util.pytorch_cos_sim(\n",
    "        sbert_model.encode(actual, convert_to_tensor=True),\n",
    "        sbert_model.encode(generated, convert_to_tensor=True)\n",
    "    ).item()\n",
    "    \n",
    "    # Calculate BERTScore (commented out)\n",
    "    # P, R, F1 = bert_score([generated], [actual], lang=\"en\", verbose=False)\n",
    "    # bert_f1_score = F1.mean().item()\n",
    "    \n",
    "    # Return single values instead of Series\n",
    "    return {\n",
    "        'BLEU': float(bleu_score),  # Ensure float return type\n",
    "        'ROUGE-1': float(rouge_scores['rouge1'].fmeasure),\n",
    "        'ROUGE-2': float(rouge_scores['rouge2'].fmeasure),\n",
    "        'ROUGE-L': float(rouge_scores['rougeL'].fmeasure),\n",
    "        'Cosine_Similarity': float(cosine_sim),\n",
    "        # 'BERTScore_F1': bert_f1_score\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each sample\n",
    "metrics_results = []\n",
    "for idx, row in samples.iterrows():\n",
    "    metrics = compute_metrics(row)\n",
    "    metrics_results.append(metrics)\n",
    "\n",
    "# Convert metrics to DataFrame and add to samples data\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "samples = samples.assign(**metrics_df)\n",
    "\n",
    "\n",
    "# # Output results for each sample and format metrics\n",
    "# for i in range(sample_size):\n",
    "#     row = samples.iloc[i]\n",
    "#     print(f\"\\nSample {i+1}:\")\n",
    "#     print(f\"User: {row['User']}\")\n",
    "#     print(f\"Actual Assistant: {row['Assistant']}\")\n",
    "#     print(f\"Generated Assistant: {row['Generated_Assistant']}\")\n",
    "#     print(\"\\nMetrics:\")\n",
    "    \n",
    "#     # Convert Series to float format for formatted output\n",
    "#     metrics = {\n",
    "#         'BLEU': float(row['BLEU'].iloc[0] if isinstance(row['BLEU'], pd.Series) else row['BLEU']),\n",
    "#         'ROUGE-1': float(row['ROUGE-1'].iloc[0] if isinstance(row['ROUGE-1'], pd.Series) else row['ROUGE-1']),\n",
    "#         'ROUGE-2': float(row['ROUGE-2'].iloc[0] if isinstance(row['ROUGE-2'], pd.Series) else row['ROUGE-2']),\n",
    "#         'ROUGE-L': float(row['ROUGE-L'].iloc[0] if isinstance(row['ROUGE-L'], pd.Series) else row['ROUGE-L']),\n",
    "#         'Cosine_Similarity': float(row['Cosine_Similarity'].iloc[0] if isinstance(row['Cosine_Similarity'], pd.Series) else row['Cosine_Similarity']),\n",
    "#         # 'BERTScore_F1': float(row['BERTScore_F1'].iloc[0] if isinstance(row['BERTScore_F1'], pd.Series) else row['BERTScore_F1'])\n",
    "#     }\n",
    "    \n",
    "#     # Output formatted metrics\n",
    "#     print(f\"BLEU Score: {metrics['BLEU']:.4f}\")\n",
    "#     print(f\"ROUGE-1: {metrics['ROUGE-1']:.4f}\")\n",
    "#     print(f\"ROUGE-2: {metrics['ROUGE-2']:.4f}\")\n",
    "#     print(f\"ROUGE-L: {metrics['ROUGE-L']:.4f}\")\n",
    "#     print(f\"Cosine Similarity: {metrics['Cosine_Similarity']:.4f}\")\n",
    "#     # print(f\"BERTScore (F1): {metrics['BERTScore_F1']:.4f}\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "metrics_columns = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine_Similarity']\n",
    "summary_stats = samples[metrics_columns].apply(lambda x: pd.to_numeric(x.iloc[0] if isinstance(x, pd.Series) else x)).describe()\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d02ff3f-b35b-4318-bec3-bb11464eb142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine_Similarity']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfaa3c4b-9476-4ef2-8dac-0596bb281c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.193598</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.146402</td>\n",
       "      <td>0.354417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029541</td>\n",
       "      <td>0.104699</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>0.083182</td>\n",
       "      <td>0.201352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.177093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.207933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.357313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.024147</td>\n",
       "      <td>0.258142</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.499388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.840896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              BLEU      ROUGE-1      ROUGE-2      ROUGE-L  Cosine_Similarity\n",
       "count  5124.000000  5124.000000  5124.000000  5124.000000        5124.000000\n",
       "mean      0.019874     0.193598     0.036133     0.146402           0.354417\n",
       "std       0.029541     0.104699     0.056829     0.083182           0.201352\n",
       "min       0.000000     0.000000     0.000000     0.000000          -0.177093\n",
       "25%       0.006497     0.122449     0.000000     0.095238           0.207933\n",
       "50%       0.013905     0.187500     0.019231     0.139535           0.357313\n",
       "75%       0.024147     0.258142     0.053333     0.185567           0.499388\n",
       "max       0.840896     1.000000     1.000000     1.000000           0.994396"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8eba3fa-2b75-4d71-b880-bb0988ee653b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "      <th>Generated_Assistant</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>User: Where is this made?\\nAssistant: Made in ...</td>\n",
       "      <td>Where is this made?</td>\n",
       "      <td>Made in USA Distributed by: Sergeant's Pet Car...</td>\n",
       "      <td>I have no idea. I have used it on my dog and s...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4847676011</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>User: Does this have an expiration date? Does ...</td>\n",
       "      <td>Does this have an expiration date? Does it onl...</td>\n",
       "      <td>yes it does have an expiration date. However, ...</td>\n",
       "      <td>I have a 10 lb. German Shepherd that is 10 yea...</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.195674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>User: Are these containers BPA free?\\nAssistan...</td>\n",
       "      <td>Are these containers BPA free?</td>\n",
       "      <td>Sorry I do not know!</td>\n",
       "      <td>Yes, they are BPA free.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004X14K</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>User: Is it airtight?\\nAssistant: Not air tigh...</td>\n",
       "      <td>Is it airtight?</td>\n",
       "      <td>Not air tight, but it clicks closed. There is ...</td>\n",
       "      <td>No. The door is open but the cat is not able t...</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.254841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00006H36X</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>User: Want for something safe for my 18 year o...</td>\n",
       "      <td>Want for something safe for my 18 year old ind...</td>\n",
       "      <td>the advantage II is a good product. have used ...</td>\n",
       "      <td>I would say go with the old ones. I think you ...</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.236220</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.234486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  4847676011                                Where is this made?   \n",
       "1  4847676011  Does this have an expiration date? Does it onl...   \n",
       "2  B00004X14K                     Are these containers BPA free?   \n",
       "3  B00004X14K                                    Is it airtight?   \n",
       "4  B00006H36X  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                            dialogue  \\\n",
       "0  User: Where is this made?\\nAssistant: Made in ...   \n",
       "1  User: Does this have an expiration date? Does ...   \n",
       "2  User: Are these containers BPA free?\\nAssistan...   \n",
       "3  User: Is it airtight?\\nAssistant: Not air tigh...   \n",
       "4  User: Want for something safe for my 18 year o...   \n",
       "\n",
       "                                                User  \\\n",
       "0                                Where is this made?   \n",
       "1  Does this have an expiration date? Does it onl...   \n",
       "2                     Are these containers BPA free?   \n",
       "3                                    Is it airtight?   \n",
       "4  Want for something safe for my 18 year old ind...   \n",
       "\n",
       "                                           Assistant  \\\n",
       "0  Made in USA Distributed by: Sergeant's Pet Car...   \n",
       "1  yes it does have an expiration date. However, ...   \n",
       "2                               Sorry I do not know!   \n",
       "3  Not air tight, but it clicks closed. There is ...   \n",
       "4  the advantage II is a good product. have used ...   \n",
       "\n",
       "                                 Generated_Assistant      BLEU   ROUGE-1  \\\n",
       "0  I have no idea. I have used it on my dog and s...  0.000000  0.000000   \n",
       "1  I have a 10 lb. German Shepherd that is 10 yea...  0.010964  0.172840   \n",
       "2                            Yes, they are BPA free.  0.000000  0.000000   \n",
       "3  No. The door is open but the cat is not able t...  0.003271  0.225352   \n",
       "4  I would say go with the old ones. I think you ...  0.029540  0.236220   \n",
       "\n",
       "    ROUGE-2   ROUGE-L  Cosine_Similarity  \n",
       "0  0.000000  0.000000           0.357362  \n",
       "1  0.000000  0.148148           0.195674  \n",
       "2  0.000000  0.000000           0.146320  \n",
       "3  0.028986  0.169014           0.254841  \n",
       "4  0.048000  0.141732           0.234486  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8224691d-ac4e-404c-bdfd-c1914b904c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_samples = samples.sort_values(by='BLEU', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17284593-b220-4cff-9d5b-51f5703456d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>User</th>\n",
       "      <th>Assistant</th>\n",
       "      <th>Generated_Assistant</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>B00164PW9S</td>\n",
       "      <td>Does this come with a thermostat? If so can it...</td>\n",
       "      <td>It does not come with a thermostat</td>\n",
       "      <td>User: Does this come with a thermostat? If so ...</td>\n",
       "      <td>Does this come with a thermostat? If so can it...</td>\n",
       "      <td>It does not come with a thermostat</td>\n",
       "      <td>It does not come with a thermostat.</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>B000ELSM1E</td>\n",
       "      <td>Can you please confirm that this item comes wi...</td>\n",
       "      <td>yes it comes with 6 tray refills</td>\n",
       "      <td>User: Can you please confirm that this item co...</td>\n",
       "      <td>Can you please confirm that this item comes wi...</td>\n",
       "      <td>yes it comes with 6 tray refills</td>\n",
       "      <td>Yes, it comes with 6 tray refills.</td>\n",
       "      <td>0.610474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>B001OE1RDK</td>\n",
       "      <td>is this product silica free?</td>\n",
       "      <td>yes is silica free.</td>\n",
       "      <td>User: is this product silica free?\\nAssistant:...</td>\n",
       "      <td>is this product silica free?</td>\n",
       "      <td>yes is silica free.</td>\n",
       "      <td>Yes, it is silica free.</td>\n",
       "      <td>0.434721</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.981865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>B007EDRCVG</td>\n",
       "      <td>Is this product made in the USA?</td>\n",
       "      <td>Yes, this product is made in the USA.</td>\n",
       "      <td>User: Is this product made in the USA?\\nAssist...</td>\n",
       "      <td>Is this product made in the USA?</td>\n",
       "      <td>Yes, this product is made in the USA.</td>\n",
       "      <td>Yes it is made in the USA</td>\n",
       "      <td>0.419174</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.922722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>B00500ITEO</td>\n",
       "      <td>What is the interior height of small size? Thanks</td>\n",
       "      <td>Hello- it is about 18\", very roomy.</td>\n",
       "      <td>User: What is the interior height of small siz...</td>\n",
       "      <td>What is the interior height of small size? Thanks</td>\n",
       "      <td>Hello- it is about 18\", very roomy.</td>\n",
       "      <td>I think it is about 18\" tall.</td>\n",
       "      <td>0.418013</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.713786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            asin                                           question  \\\n",
       "1624  B00164PW9S  Does this come with a thermostat? If so can it...   \n",
       "939   B000ELSM1E  Can you please confirm that this item comes wi...   \n",
       "2014  B001OE1RDK                       is this product silica free?   \n",
       "3912  B007EDRCVG                   Is this product made in the USA?   \n",
       "3363  B00500ITEO  What is the interior height of small size? Thanks   \n",
       "\n",
       "                                     answer  \\\n",
       "1624     It does not come with a thermostat   \n",
       "939        yes it comes with 6 tray refills   \n",
       "2014                    yes is silica free.   \n",
       "3912  Yes, this product is made in the USA.   \n",
       "3363    Hello- it is about 18\", very roomy.   \n",
       "\n",
       "                                               dialogue  \\\n",
       "1624  User: Does this come with a thermostat? If so ...   \n",
       "939   User: Can you please confirm that this item co...   \n",
       "2014  User: is this product silica free?\\nAssistant:...   \n",
       "3912  User: Is this product made in the USA?\\nAssist...   \n",
       "3363  User: What is the interior height of small siz...   \n",
       "\n",
       "                                                   User  \\\n",
       "1624  Does this come with a thermostat? If so can it...   \n",
       "939   Can you please confirm that this item comes wi...   \n",
       "2014                       is this product silica free?   \n",
       "3912                   Is this product made in the USA?   \n",
       "3363  What is the interior height of small size? Thanks   \n",
       "\n",
       "                                  Assistant  \\\n",
       "1624     It does not come with a thermostat   \n",
       "939        yes it comes with 6 tray refills   \n",
       "2014                    yes is silica free.   \n",
       "3912  Yes, this product is made in the USA.   \n",
       "3363    Hello- it is about 18\", very roomy.   \n",
       "\n",
       "                      Generated_Assistant      BLEU   ROUGE-1   ROUGE-2  \\\n",
       "1624  It does not come with a thermostat.  0.840896  1.000000  1.000000   \n",
       "939    Yes, it comes with 6 tray refills.  0.610474  1.000000  1.000000   \n",
       "2014              Yes, it is silica free.  0.434721  0.888889  0.571429   \n",
       "3912            Yes it is made in the USA  0.419174  0.800000  0.615385   \n",
       "3363        I think it is about 18\" tall.  0.418013  0.571429  0.500000   \n",
       "\n",
       "       ROUGE-L  Cosine_Similarity  \n",
       "1624  1.000000           0.958470  \n",
       "939   1.000000           0.994396  \n",
       "2014  0.888889           0.981865  \n",
       "3912  0.800000           0.922722  \n",
       "3363  0.571429           0.713786  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97266506-7d0e-47be-b7eb-9077fe1ac72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
